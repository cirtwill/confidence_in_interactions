\documentclass[12pt]{article}
\usepackage{amsmath} 
\usepackage[dvips]{graphicx}
\usepackage{multirow} 
\usepackage{geometry} 
\usepackage{pdflscape}
\usepackage[labelfont=bf]{caption} 
\usepackage{setspace}
\usepackage[running]{lineno} 
% \usepackage[numbers,sort]{natbib}
\usepackage[round]{natbib} 
\usepackage{array}
\usepackage{hyperref,url}
\usepackage{float}
\usepackage{mdframed}
\makeatletter


% this creates a custom and simpler ruled box style
\newcommand\floatc@simplerule[2]{{\@fs@cfont #1 #2}\par}
\newcommand\fs@simplerule{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@simplerule
  \def\@fs@pre{\hrule height.8pt depth0pt \kern4pt}%
  \def\@fs@post{\kern4pt\hrule height.8pt depth0pt \kern4pt \relax}%
  \def\@fs@mid{\kern8pt}%
  \let\@fs@iftopcapt\iftrue}

% this code block defines the new and custom mdframed environment
\newmdenv[rightline=true,bottomline=true,topline=true,leftline=true,linewidth=2pt]{fullbox}


\newcommand{\methods}{\textit{Materials \& Methods}}
\newcommand{\SI}{\textit{Appendix}~}

\topmargin -1.5cm % 0.0cm 
\oddsidemargin 0.0cm % 0.2cm 
\textwidth 6.5in
\textheight 9.0in % 21cm
\footskip 1.0cm % 1.0cm

\usepackage{authblk}

\title{A quantitative framework for investigating the reliability of network construction}


\author{Alyssa R. Cirtwill$^{1\dagger}$, \&  Anna Ekl\"{o}f$^{1}$, Tomas Roslin$^{2}$, Kate Wootton$^{2}$, Dominique Gravel$^{3}$}
\date{
% \begin{minipage}[h]{0.6\textwidth}
\small$^1$ Department of Physics,\\
Chemistry and Biology (IFM)\\ 
Link\"{o}ping University\\
Link\"{o}ping, Sweden\\
\medskip
\small$^2$ Department of Ecology\\ 
P.O. Box 7044\\ 
Swedish University of Agricultural Sciences \\ 
SE-750 07 Uppsala, Sweden\\
\medskip
\small$^3$ D\'{e}partement de biologie\\ 
Universit\'{e} de Sherbrooke\\ 
Sherbrooke, Canada
\medskip
\small$^\dagger$ Corresponding author:\\
alyssa.cirtwill@gmail.com\\
\medskip
\medskip
\normalsize Running head: Constructing quantitative interaction networks
% tel: +46 723 158464\\
% $^\ddagger$ anna.eklof@liu.se\\
% $^\star$ tomas.roslin@slu.se\\
% $^\diamond$ kate.wootton@slu.se\\
% $^\triangleright$ dominique.gravel@usherbrooke.ca
}


\renewcommand\Authands{ and }

\begin{document} 
\maketitle 
\raggedright
\setlength{\parindent}{15pt} 

\vspace{-.4in}

% {\small

% \section*{\small Details}

% \begin{minipage}[h]{0.6\textwidth}
% \begin{itemize}
% \item Running title: Quantitative network construction
% \item Article type: Ideas and Perspectives
% \item Number of references: 36
% \item Number of figs, tables, \& text boxes: 8 % Limit is 10
% \end{itemize}
% \end{minipage}\begin{minipage}[h]{0.4\textwidth}
% \begin{itemize}
% \item Text box 1 word count: 381
% \item Text box 2 word count: 210
% \item Abstract word count: 199
% \item Main text word count: 6482 % Including 18 page numbers. % No abstract, acknowledgements, references, table or figure legends. Limit is 6000-7000 including figs, tables, references. approx. 10K all in :/
% \end{itemize}
% \end{minipage}
% }

\newpage

\begin{spacing}{2.0}

\section*{Abstract}

  \begin{enumerate}

    \item  Descriptions of ecological networks typically assume that the same interspecific interactions occur each time a community is observed. This contrasts with the known stochasticity of ecological communities: community composition, species abundances, and link structure all vary in space and time. Moreover, finite sampling generates variation in the set of interactions actually observed. 
    \item Here we develop the conceptual and analytical tools needed to capture uncertainty in the estimation of pairwise interactions. To define the problem, we identify the different contributions to the uncertainty of an interaction and its implications for the estimation of network properties. We then outline a framework to quantify the uncertainty around each interaction. We illustrate this framework using the most extensively sampled network to date. 
    \item We found significant uncertainty in estimates for the probability of most pairwise interactions which we could, however, limit with informative priors. This uncertainty scaled up to summary measures of network structure such as connectance or nestedness. Even with informative priors, our results suggest that the relatively well-sampled network we use as a test case is missing many interactions that may occur rarely or under different local conditions. 
    \item Through these efforts, we demonstrate the utility of our approach and the importance of acknowledging the uncertainty inherent in network studies. Most importantly, we stress that networks are best thought of as systems constructed from random variables, the stochastic nature of which must be acknowledged for an accurate representation. Doing so will fundamentally change networks analyses and yield greater realism.
  % Can be up to 350 words for MEE. This is 200 exactly.
\end{enumerate}


\section*{\small Keywords}

ecological networks; probabilistic interactions; Bayesian networks; sampling error; spatial variability; temporal variability; uncertainty

% \linenumbers
\clearpage

\section*{Introduction}

    Representing an assemblage of species as a network offers a convenient summary of how the community is constructed as networks simultaneously describe species composition and interactions between species. A tabulation of the nodes (species) and their relative abundances forms the basis for traditional metrics of community composition such as alpha diversity. To move from these simpler metrics to a network framework, the tabulation of nodes is combined with interactions (links between nodes) so that networks provide additional, higher-order information on community structure. Despite this additional information, empirical descriptions of ecological networks are still limited because they are usually considered static representations of the communities they describe. That is, whether the network is assembled based on aggregated data, a single intensive ``snapshot" sample, or expert knowledge, interactions are assumed to occur deterministically wherever and whenever the community is observed~\citep{Olesen2011a}. 


    The assumption of static communities contrasts significantly with the known stochasticity of ecological communities~\citep{Gotelli2000}. Community composition and species abundances vary over space~\citep{Baiser2012} and time~\citep{Olesen2011a} while interactions vary over space~\citep{Kitching1987,Baiser2012}, time~\citep{Kitching1987,Olesen2011a}, and between individuals~\citep{Pires2011a,Fodrie2015,Novak2015}. Moreover, variability in community composition and interactions may or may not be closely related. The removal of a species from a site will obviously also remove its interactions but, conversely, the co-occurrence of potentially interacting species does not guarantee that the species will interact at a given place and time. Interactions can be lost if the interaction partners remain present but are separated in time or are too rare to detect each other~\citep{Tylianakis2010}. Interactions can also fail to occur because of environmental contingencies~\citep{Poisot2015}, or through changes to individual preferences~\citep{Fodrie2015}. 


    Beyond ``true" variation in network structure, several researchers have noted the effects of sampling intensity on network structure (e.g.,~\citealp{Martinez1999,Bluthgen2006,Bluthgen2007}). An assessment of the accumulation of interactions with increasing sampling effort suggests that it is even more challenging to document interactions than species~\citep{Poisot2012}. As a result, it has been proposed that interactions should be described probabilistically and network metrics computed accordingly~\citep{Poisot2016}. Early work in this vein includes food-web models using likelihood-based approaches~\citep{Allesina2008} or Gaussian~\citep{Williams2010} or binomial~\citep{Rohr2016} probability functions for each possible interaction. These models may include information about species' traits~\citep{Rohr2016} or may attempt to reproduce empirical network structures using a set of simple rules~\citep{Allesina2008,Williams2010}.


    Despite these preliminary efforts, we currently lack the quantitative methodology to deal with the uncertainty generated by spatiotemporal variation in ecological interactions and by sampling. Even in extremely well-sampled networks, uneven sampling across species (or pairs of species) can lead to the erroneous inference that some species do not interact because they co-occur rarely or have not yet been observed together - even if they do interact when they do co-occur (see Fig.~\ref{histograms} for an example). Nearly all network studies will thus neglect some interactions, necessitating an approach that acknowledges this uncertainty.


    In this study, we formalise the description of interactions between species as probabilities and develop analytical tools to capture the uncertainty in the estimation of these interactions. We focus on binary interactions as a first step, but the framework could be expanded to deal with interaction frequencies and strength. To define the problem, we will first identify the different contributions to the uncertainty of an interaction and discuss the implications of each source of uncertainty for the properties of ecological networks. 
    % Next, we develop an analytical framework to quantify the uncertainty around interactions in an empirical web and, finally, illustrate this framework using an extensively sampled empirical network.
    % We illustrate this framework using the most extensively sampled network to date (Box 1).  Finally, we offer tangible recommendations for improved descriptors of ecological interactions. Through these efforts, we demonstrate both the utility of our approach and the importance of acknowledging the uncertainty inherent in network studies.


    \subsection*{Why do some interactions \emph{not} occur?}

      To define the problems associated with quantifying ecological interaction networks, we will start from the perspective of an empirical community ecologist faced with the task of describing a previously unknown interaction network. This ecologist will be interested in generating a description of the species/nodes present and the links between them~\citep{Roslin2016}.  Importantly, the information sought is conveyed by both the presence and \emph{absence} of links. Presences and absences are not, however, equally certain. An observed link definitely occurred, but there are multiple reasons why a given link may not be observed \emph{whether or not the interaction truly occurred}. The detection of any interaction is a stochastic process. We define three nested levels of uncertainty contributing to this stochasticity: interaction uncertainty, process uncertainty, and detection uncertainty.


        \subsubsection*{Interaction uncertainty} 

          First, and most fundamentally, we do not know whether or not a pair of species have the appropriate characteristics (or traits) to interact. We define the probability of an interaction $L$ given characteristics $\mathbf{T}$ as $P(L | \mathbf{T})=\lambda$. If two species have incompatible traits, they may not interact even if there are no other constraints (e.g., temporal separation of species, inability to detect cryptic species) preventing the interaction from being observed. As a simple example, a prey species may be too large to be consumed by a predator. In such cases, $\lambda$ would take a value of 0 and there would be no uncertainty. 

          Based on an observed network, however, it is often unclear whether an unobserved interaction is impossible due to incompatible traits or simply a rare phenomenon with $\lambda>0$. This uncertainty is partly addressed by trait-matching models~\citep{Bartomeus2016}. Nevertheless, every model is imperfect and lacks information that could be used to define constraints on interactions~\citep{Dormann2017}. Some interaction uncertainty therefore remains in current trait-matching models. As more information about the influence of traits on interactions is uncovered, this uncertainty is likely to be reduced and $\lambda$ should either tend to 0 or to 1. 


        \subsubsection*{Process uncertainty} 

         Even when an interaction is feasible, i.e. $L=1$, it may still not occur at a given location or moment in time because of local constraints such as inclement weather or the lack of suitable habitat. We define the local realisation of an interaction, $X$, given that the interaction is feasible, as a stochastic process with associated probability $P(X|L=1)=\chi$. This phenomenon of interaction contingencies is usually not considered in network studies, but there is a rich literature in community ecology describing the contingencies of interactions. Phenological matching~\citep{MillerRushing2010,Gezon2016}, species preferences~\citep{Pires2011,Novak2015,Coux2016}, and fear effects of other species~\citep{Luttbeg2005,Wirsing2008} are just some of the factors contributing to variation in the frequency of interactions between a given pair of species. Some of the factors leading to process uncertainty can be addressed in mesocosm studies of networks (e.g., environmental conditions can be held stable) or could be included in models along the line of trait-matching models (e.g., phenological matching). In the field, however, process uncertainty is likely inevitable.


        \subsubsection*{Detection uncertainty} 

          Lastly, measurement errors are a pervasive source of uncertainty in the observation of ecological processes. Given that an interaction is feasible and occurs under the local conditions ($L$=1 and $X$=1), we may define the detection of an interaction, $D$, as a stochastic process with the associated probability $P(D|X=1,L=1)=\delta$. Detection failure could happen for many reasons (see~\citet{Wirta2014} for examples of these difficulties and partial solutions to them). Some sources of detection error can be minimised with appropriate sampling effort ($\delta$ will converge to one with increasing number of samples), but other sources are often difficult to reduce (e.g. the occurrence of cryptic species might require molecular analysis for appropriate taxonomic identification as in~\citealt{Wirta2014,Frost2016}). 



    \subsection*{Estimating detection and process uncertainty}

        An observed interaction indicates that an interaction was feasible, occurred at the study site during sampling, and was detectable. That is, $L = 1$, $X = 1$, and $D = 1$. An unobserved interaction could be due to any of the three sources of uncertainty ($D$, $X$, and/or $L = 0$). Only the case where $L = 0$ is a true absence of interaction --the usual interpretation of a $0$ value in an interaction matrix. It is particularly important to rule out the situations where $D=0 \cup X = 1 \cup L=1$, i.e. where the interaction occurred at the location but was not observed, and $X = 0 \cup L =1$, i.e., where the interaction is feasible but did not occur at the local site. 
        % The occurrence of a true absence, our quantity of interest, corresponds to the joint event $L=0 \cup X=1 \cup D=1$ but %% AC: Doesn't a true absence just mean $L=0$ regardless of our ability to detect/local conditions? 
        Using empirical data, however, it is impossible to distinguish between sources of variation. An empirical ecologist will measure the marginal probability $P(L) = k/n$, where $k$ is the number of observed interactions and $n$ the number of observed co-occurrences. This probability does not contain any information about $D$ or $X$.


        The considerations above raise major challenges: when faced with empirical data, how may we infer whether unobserved interactions truly do not occur? How may we refine our sampling approaches to reduce uncertainties and gain insights into the impact of multiple processes on field observations? Importantly, some sources of uncertainty within these three broad groupings can be minimised with appropriate sampling (e.g., sampling in a variety of weather conditions, combining different methods of detection) while other sources are difficult or impossible to reduce since they are generated by the process in which we are interested (e.g., variation in individual preferences or traits). Given this multifaceted problem of uncertainty, what can we do to separate the different types of variation and reduce those that can be reduced?


        When attempting to reduce uncertainty, the obvious rule of thumb is to ``sample more" (Fig.~\ref{upper_limits}). Increasing sample sizes will reduce uncertainty about the upper bound of the interaction probability and will also increase the probability of detecting unlikely or cryptic interactions (e.g., interactions where $L$=1 but process or detection uncertainty is high). Despite these benefits, we note that there are limits to the utility of increased sampling. Since the probability of observing the co-occurrence of two species will always be higher than the probability of observing their interaction (since the probability of interaction is conditional on both interaction partners being present), we will accumulate observations of co-occurrences faster than we will accumulate observations of interactions (Fig.~\ref{histograms}C). Thus, the more we sample, the more zeros will appear in our interaction matrix.


        An added complication is that increased sampling will not reduce uncertainty evenly across interactions. To record an interaction between A and B, we need to identify both partners correctly (a non-trivial problem in many food webs; e.g.~\citealp{Kaartinen2011,Roslin2016}) and be able to resolve all interactions with a similar likelihood. For both molecular and rearing techniques, certain types of interactions may go unnoticed due to technical challenges~\citep{Wirta2014}. This can bias the set of recorded interactions towards those that are easier to observe.


        A slight variation on the theme of ``sample more" is to sample repeatedly. In one endeavour to uncover the cause of unobserved interactions,~\citet{Weinstein2017} used repeated sampling rounds to estimate the daily probability of detecting an interaction and thereby model detection and process uncertainty. While conceptually attractive, this approach is unsuitable for interactions occurring over longer time scales (e.g., associations between hosts and parasitoids with a single generation per year), or very rare interactions which might not occur on any of the sampling days. Worse, the problem persists that the absence of an interaction on a given day could be because it was impossible on that day despite being otherwise feasible [$P(X|L=1)=0$], because interaction did occur but could not be observed [$P(D|X=1,L=1)=0$]. From a conceptual perspective, this approach therefore fails to satisfactorily distinguish between sources of uncertainty. 


        Most importantly, if two species are never observed co-occurring during several days of sampling then repeated sampling reveals nothing about their probability of interacting if they should ever co-occur. The bottom line is that separating different sources of uncertainty is difficult indeed. Nevertheless, we propose that information on the probability of detecting interactions in a focal system can be obtained by combining repeated sampling with prior information from the same or a similar system. Here, we first outline the statistical framework for a Bayesian approach to probabilistic interactions and then illustrate this approach with an intensively and repeatedly sampled empirical dataset.


\section*{Materials and methods}


  \subsection*{A naive quantification of uncertainty}

    We start by considering how to naively quantify an interaction probability and its associated uncertainty \emph{for an interaction that has not yet been observed}. Consider the case where a pair of species have been observed co-occurring $n$ times, of which they have been observed to interact in $k = 0$ cases. To evaluate the uncertainty of this interaction, consider the occurrence of an interaction as a Bernoulli trial. In this framework, the number of successes $k$ over $n$ trials will follow a binomial distribution: 
        
        \begin{equation}
          L \sim Bin(n,\lambda) ,
        \end{equation}


        \begin{equation}
           P(L = k|\lambda,n) = {n \choose k}\lambda^k(1-\lambda)^{n-k} . 
           \label{likelihood}
        \end{equation}

    \noindent The parameter $\lambda$, the probability of observing an interaction over an infinite time interval and area, is the quantity we want to estimate from empirical data. 
    The maximal likelihood estimate (MLE) of $\lambda$ is straightforward to find given $k$ and $n$:

        \begin{equation}
          \lambda_{MLE} = \frac{k}{n}  .
          \label{theta_MLE}
        \end{equation}


    Note that $\lambda$ is not a single point estimate but rather a random variable with an unknown distribution. This means that if $k = 0$ in a given sample, this does not necessarily imply that the two species will never interact. Rather, $k = 0$ implies that `no interaction' is the most likely outcome when the species do co-occur but there is nonetheless some chance that the two species \emph{could} interact. In the situation where $k>0$, in contrast, we are sure that the interaction is feasible ($L = 1$). If $n>k>0$, the interaction is feasible but there may be local constraints ($X = 0$) or detection errors ($D<1$) causing the interaction not to be observed in some samples. 


    To properly interpret $\lambda$, we need to estimate the variance as well as the MLE. The variance of a Bernoulli experiment is $n\lambda$(1-$\lambda$) and, importantly, describes the variability of the number of successes $k$ for $n$ trials rather than the variance associated with the estimation of $\lambda$. It is, however, straightforward to compute the confidence interval for the MLE of $\lambda$ using any of several methods, including the \emph{Wilson score interval}, the \emph{Clopper-Pearson interval}, and the \emph{Agresti-Coull interval} (for details, see [\citealp{Brown2001}]). % The non-point nature of lambda seems like a feature, not a bug. 


    All of the above methods for estimating the variance of $\lambda$ include the number of samples $n$. This means that where the number of samples $n$ is very low, as for rare species, there will be considerable uncertainty around our estimate of $\lambda$. In Fig.~\ref{upper_limits}, we derive the Clopper-Pearson interval to explore how the estimate of $\lambda$ varies with sample size. At a small sample size, the 95\% confidence interval spans all values of $\lambda$. To establish that species are not interacting with any acceptable certainty requires tens of observations of the two species co-occurring but not interacting. As most data sets will lack such extensive sampling across all species pairs, we can use a Bayesian approach to supplement the data we have with other sources of information.


  \subsection*{Inferring interaction probabilities}

      \subsubsection*{Posterior distribution of the interaction probability}

        Here we adopt a Bayesian approach to estimate the posterior distribution of the parameter $\lambda$ :

        \begin{equation}
          \underbrace{P(\lambda|k,n)}_{Posterior} = \frac{\overbrace{P(k|\lambda,n)}^{Likelihood}\overbrace{P(\lambda)}^{Prior}}{\underbrace{P(k|N)}_{Normaliser}} .
          \label{posterior}
        \end{equation}

        According to the above description, the likelihood is simply the binomial distribution (Eq.~\ref{likelihood}). Since $\lambda$ is a probability, it is bounded between 0 and 1 and the most appropriate prior distribution is the beta:

        \begin{equation}
          \lambda \sim Beta(\alpha,\beta) , \label{prior}
        \end{equation}

        \noindent which has two shape parameters, $\alpha$ and $\beta$. 

       The beta-binomial distribution is a conjugate distribution of the binomial distribution. This allows us to analytically compute the posterior distribution of a binomial model with a beta prior distribution. We can re-write the posterior distribution of $\lambda$ as:

        \begin{equation}
          P(\lambda|k,n) = \frac{\lambda^{\alpha+k-1}(1-\lambda)^{\beta+n-k-1}}{B(\alpha+k,\beta+n-k)} , \label{posterior2}
        \end{equation}

        
        \noindent where the function $B$ is the beta function. The posterior distribution of $\lambda$ therefore follows the beta distribution with new parameters $\alpha'= \alpha+k$ and $\beta'=\beta+n-k$. The weight of the prior on the posterior distribution can be understood from these parameter definitions: the difference between the posterior and the prior will increase with $k$ and $n-k$. In other words, the distribution of $\lambda$ for better-sampled pairs of species will rely less on the information used to build the prior distribution and depend more on the observed data.
        When plotted, we find the shape of the distribution gets narrower with $k$ and $n$ (Fig.~\ref{Salix_pdfs_cdfs}). 


      \subsubsection*{Moments and other properties}

        It is common to preform analyses that require calculating higher-order network properties in interaction networks. The fact that the posterior distribution of $\lambda$ follows a beta distribution makes it straightforward to compute moments and other properties needed for this. 


        The \textbf{average} of $\lambda$ is: 
            \begin{equation}
              \bar{\lambda} = \frac{\alpha+k}{\alpha+\beta+n} ,
              \label{mean}
            \end{equation}

          and its \textbf{variance} is:  
            \begin{equation}
              Var(\lambda|k) = \frac{(\alpha + k)(\beta + n - k)}{(\alpha + \beta + n)^{2}(\alpha + \beta + n +1)}
              \label{variance}
            \end{equation}

          The \textbf{mode} of the distribution is:
            \begin{equation}
              \hat{\lambda} = \frac{\alpha + k - 1}{\alpha + \beta + n - 2} .
              \label{mode}
            \end{equation}


      \subsubsection*{The prior distribution}    

        Parameters $\alpha$ and $\beta$ determine the shape of the prior distribution of $\lambda$, which follows a beta distribution. These are called hyper parameters. The prior may be chosen to be uninformative, if no external information is available, or may draw on information such as distributions of connectance across webs, degree within webs, or a trait-matching function (see \emph{Appendix S1} for details and a simple quantitative example).


\subsection*{Scaling up to networks - an empirical example}


    In the following section, we will provide an empirical example based on the well-sampled system of willow (\emph{Salix}) species, herbivorous gallers, and their natural enemies described by~\citet{Kopelke2017}. This dataset consists of a single community type sampled across Europe over 29 years and at 374 unique locations. The data were collected over 29 years at 374 unique locations across Europe. The meta-network consists of 1,173 different interactions between 52 \emph{Salix} nodes, 92 herbivore nodes, and 126 natural enemy nodes (see \emph{Appendix S2} for details). 


    The high spatiotemporal resolution of this network and the unusually high sampling effort implemented at the site level makes this dataset particularly well suited for illustrating the difficulties in completely sampling a network and testing Bayesian approaches to overcome these difficulties. To do this, we first compared the frequencies of observed co-occurrences and interactions. We analysed both the \emph{Salix}-galler and galler-natural enemy components of the network but, for the sake of brevity, present only the latter here. For the \emph{Salix}-galler component see \emph{Appendix S3}.


  \subsection*{Computing the prior and posterior distributions}

      In a strict Bayesian framework, we wish to use a prior distribution that does not rely on any information from the study at hand. Network data for a similar study system may, however, not be available. In that case, one might use the first sub-network collected as ``training data" to guide future sampling. To simulate this situation, we created priors using a single sub-network from the middle of the geographical distribution of the~\citet{Kopelke2017} dataset. To demonstrate how the use of data from a different system can affect the prior distribution and conclusions based on it, we repeated our analyses using priors derived from a much smaller \emph{Salix}-galler-natural enemy system~\citep[Data available from the Dryad Digital Repository: https://doi.org/10.5061/dryad.g7805]{Barbour2016}\nocite{Barbour2016Dryad}. This smaller system was much more densely-connected than that described in~\citet{Kopelke2017} and provided unreasonable distributions for interaction probabilities (\emph{Appendix S4}). 


      To obtain the priors based on the Zillis sub-network, we estimated frequencies of interactions based on the normalised degree of each species in each network component
      (see \emph{Appendix S5} for details and code). Using these prior parameters, we then estimated the posterior distributions of interaction probabilities $\lambda$. For species without observed interactions ($n = 0$), the posterior distribution is identical to the prior distribution. For species where $n>0$, we can update the prior distribution with data. If we consider only pairs of species which were observed to co-occur but not to interact, $k_{ij}$ is always 0 and only $n_{ij}$ will vary between species pairs. This gives $\alpha'$=$\alpha$ and $\beta'$=$\beta + n_{ij}$. We calculated posterior distributions and 95\% credible intervals (see function ``credible\_interval" in \emph{Appendix S6}) for species with $n$ ranging between 0 and 374, the total number of sites in our dataset. 


  \subsection*{How many samples are required to reach a minimal precision}

      Rather than calculating credible intervals for a posterior distribution after collecting data, we may wish to know how many data points are necessary to obtain a given level of confidence that two co-occurring species do not interact. The number of samples needed will depend on both our desired level of confidence and the threshold below which we assume that two species are unlikely to ever interact (Fig.~\ref{Salix_pdfs_cdfs}; see function samples\_for\_threshold in~\emph{Appendix S6}). The number of samples required to be 95\% confident that the interaction probability between galler and natural enemy species is below a threshold increases quickly as the threshold decreases (\emph{Appendix S7}). The 95\% credible interval is (\textless0.001, 0.303) for the probability of interaction between two species observed to co-occur but never interact. To be 95\% confident that the probability of interaction is below 0.1, 0.05, or 0.01 would require 15, 39, and 229 observed co-occurrences, respectively.


      Given the low levels of replication in most network studies, this implies that we should have fairly low confidence in many ``non-interacting" pairs of species. Even in the extensively replicated \emph{Salix}-galler-natural enemy dataset, very few species pairs were observed co-occurring frequently enough to reach these thresholds.  Regardless of our choice of prior, no species pairs were observed to co-occur frequently enough to reach the threshold for an interaction probability of 0.01. Discounting potential interactions, then, requires either a stronger prior expectation of no interaction (e.g. for forbidden interactions) or very extensive sampling. For all we know, most links absent from current descriptions of network structure may be so not because the species do not interact, but because we have not sampled deeply enough to detect them.


  \subsection*{Scaling up to network metrics}

    It is straightforward to compute most network metrics when the different $\lambda$ of the adjacency matrix are known and assumed not to vary~\citep{Poisot2016}. Incorporating variance in $\lambda$ into these calculations, however, is not so easy. Computation of these metrics involves non-linear functions, and Jensen's inequality states that the average of a non-linear function of a stochastic variable differs from the function of the average of that variable. Any uncertainty in the values of $\lambda$ could therefore bias both the mean and variance of a network metric. One way to avoid potentially biased analytical calculation of network properties is to calculate the properties of a suite of simulated networks.


    Using the prior distributions and procedures described above, we calculated posterior probability distributions for species pairs that were not observed interacting. Using these posterior distributions and assuming probabilities of 1 for pairs of species that were observed interacting, we created a suite of 100 webs by randomly sampling from each posterior distribution. After obtaining these posterior networks, we calculated the connectance of each web, as well as the mean number of links per galler and per natural enemy, and the nestedness (NODF) of the network. To demonstrate how these network metrics will be affected by detection uncertainty, we then created a suite of filtered networks for each posterior network. Networks were filtered by randomly sampling 99\%, 95\%, 90\%, 80\%, 70\%, 60\%, and 50\% of the interactions included in each posterior network. This gradient is akin to a gradient of sampling effort. For each level of detection accuracy, we created 100 randomly-sampled networks per posterior-probability network (giving 100 posterior networks and 1000 detection-filtered networks). We calculated the same network properties as described above for all posterior and detection-filtered networks.


    We found that the connectance, mean links per galler, and mean links per natural enemy were much lower in the observed web (C=0.078, $L_{galler}$=9.99, and $L_{natural enemy}$=7.45, respectively) than in the posterior webs (0.186 $\leq$ C $\leq$ 0.198, 13.4 $\leq L_{galler} \leq$ 14.6, and 23.4 $\leq L_{natural enemy} \leq$ 25.0). When the detection probability was relatively low (i.e., 50\%), however, the properties of randomised networks became similar to those in the observed webs (Fig.~\ref{posterior_webs}A,B,D). Nestedness was higher in the observed network (NODF=6.85) than in the posterior webs (6.31 $\leq NODF \leq$ 6.82; Fig.~\ref{posterior_webs}C); in this case, the stronger the detection filter the farther apart were the observed and posterior webs. 


\section*{Results}


  Most pairs of species (9,794/12,096 galler-natural enemy pairs) are never found co-occurring and, for species that did occur together, the total number of co-occurrences was generally low (mean=3.87, variance=28.8; Fig.~\ref{histograms}A). The bulk of these co-occurring species pairs were never observed to interact: only 7.76\% of galler-natural enemy pairs were observed interacting. Of those pairs that did interact, the incidence of interaction was also low (mean=4.04, variance=29.3; Fig.~\ref{histograms}B). Moreover, even when a pair of species was observed to interact, the number of interactions was often lower than the number of co-occurrences (Fig.~\ref{histograms}C).


  \subsection*{Computing the prior and posterior distributions}


      We obtained prior parameters of $\alpha$=0.700, $\beta$=8.49. Where $n = 0$, these parameters gave a posterior distribution with $\bar\lambda$=0.076, var($\lambda$)=0.008. The posterior interaction probabilities obtained based on the Zillis sub-network were much lower than those obtained based on~\citet[Data available from the Dryad Digital Repository: https://doi.org/10.5061/dryad.g7805]{Barbour2016}; this emphasises the importance of using an appropriate study system when constructing a prior (\emph{Appendix S4}). When $n = 374$, we obtained a posterior distribution with $\bar\lambda_{ij}$=1.83 $\times$ 10$^{-3}$, var($\lambda_{ij}$)=4.76. This distributions is very close to 0 with small variance about $\lambda$; species $i$ and $j$ are extremely unlikely to interact at sites or times not included in our sample.


      For most pairs of species $i$ and $j$, however, $n_{ij}$ was much less than 374 and our posterior mean and variance therefore retain more of the influence of the prior. We can see this in the increasing means and variances as we decrease $n_{ij}$ (Fig.~\ref{Salix_pdfs_cdfs}). The change in distribution as $n_{ij}$ decreases can also be shown  by calculating 95\% credible intervals for $\lambda$ (see the function ``credible\_interval" in \emph{Appendix S6}). The 95\% credible interval for hypothetical galler-natural enemy pairs, meanwhile, widened from (0.00001, 0.008) to (0.0005, 0.304).




  \subsection*{How many samples are required to reach a minimal precision}


  \subsection*{Scaling up to network metrics}


\section*{Discussion}

  Even in the most extensive data set that we could find, there was very little empirical data for each species pair. Most pairs of species were not observed co-occurring even once, and less than 10\% of species pairs were observed interacting. This suggests that limited sampling is a major source of uncertainty in all empirical networks. This dataset also illustrates the problem of process uncertainty - pairs of species which were observed to interact did not interact in all of the samples in which they co-occurred. The number of observed co-occurrences strongly affected the posterior distribution of $\lambda$, meaning that our level of confidence that an interaction probability is below some threshold depends strongly on the amount of information available for each pair of species.




  [[Recap results]]


  Real interaction networks vary over several dimensions~\citep{Kitching1987,Olesen2011a,Pires2011a,Baiser2012,Fodrie2015,Novak2015} and to capture this variation we must turn from static descriptions of network structure to probabilistic descriptions. In this study, we have developed the analytical tools to capture the uncertainty in the estimation of pairwise interactions and a conceptual framework for its individual components: interaction uncertainty, process uncertainty, and detection uncertainty. Using this framework leads us to offer tangible recommendations for improved descriptors of ecological interactions. First, our analyses point to detection uncertainty as a major contributor to overall uncertainty of is establishing the absence of interaction. To counter this and establish true absences of interactions requires comparatively large sample size â€“ on the order of 30-50 observations per species pair. Second, where such extensive sampling is not feasible, researchers should still acknowledge the varying levels of confidence surrounding the presence or absence of interactions between different pairs of species. Including the $n$ and $k$ values for each interaction will clearly indicate which unobserved interactions are most likely to be observed with further sampling and which estimates are more reliable. Third, the uncertainty around interactions (especially interactions that were not observed) should be incorporated in calculations of network properties like connectance or nestedness. Re-sampling networks based on a probabilistic understanding of networks is straightforward and gives distributions for network properties rather than point estimates. This not only acknowledges the fact that interactions vary over time and space but will also facilitate comparisons between networks. With confidence intervals around network metrics, we can not only say that one network is more connected than another but also whether the networks are more different than we would expect based on imperfect sampling of interactions. To facilitate these recommendations, we provide all code used in this paper in the supplementary material. 


\section*{Acknowledgements}

  The authors thank Daniel Cartensen for fruitful discussion of the ideas in this manuscript and K\'{e}vin Cazalles for providing feedback. The authors appreciate support from the Swedish Research Council (VR) for grant \#2016-06872 (to TR) from Formas for grant \#942-2015-1262 (to AE).


\section*{Authors' contributions}

DG designed the analytical approach. AC and DG wrote the code. AC performed statistical analyses. All authors contributed to writing and revising the manuscript.


\section*{Data accessibility}

All data used in this study have been independently published and are accessible following the references provided in text.



\end{spacing}
\clearpage

\captionsetup{singlelinecheck=false, font={stretch=1}}

\section*{Boxes and figures}


% \begin{fullbox}{}
%   \begin{spacing}{1.0}
%     \textbf{Box 1: }\emph{Salix}-galler-natural enemy dataset.\\
%     \indent As a case study, we use an extensively sampled \emph{Salix}-galler-natural enemy meta-network. This dataset consists of a single community type sampled across Europe: willow (\emph{Salix}) species, willow-galling sawflies, and their natural enemies. The data were collected over 29 years at 374 unique locations across Europe. The meta-network consists of 1,173 different interactions between 52 \emph{Salix} nodes, 92 herbivore nodes, and 126 natural enemy nodes. The high spatiotemporal resolution of this network and the unusually high sampling effort implemented at the site level makes this dataset particularly well suited for illustrating the difficulties in completely sampling a network and testing Bayesian approaches to overcome these difficulties.\\
%     \indent We may begin by comparing the frequencies of co-occurrences and interactions to reveal the challenge of having sufficient sampling to be confident that an interaction does not occur. Most pairs of species (9,794/12,096 galler-natural enemy pairs) are never found co-occurring and, for species that did occur together, the total number of co-occurrences was generally low (mean=3.87, variance=28.8; Fig.~\ref{histograms}A). The bulk of these co-occurring species pairs were never observed to interact: only 7.76\% of galler-natural enemy pairs were observed interacting. Of those pairs that did interact, the incidence of interaction was also low (mean=4.04, variance=29.3; Fig.~\ref{histograms}B). Thus, even in the most extensive data set that we could find, there was very little empirical data for each species pair. This suggests that limited sampling is a major source of uncertainty in all empirical networks. This dataset also illustrates the potential for increased sampling to not necessarily reveal more interactions as a pair of species that is able to interact may not be observed interacting in all samples where the pair co-occurs (Fig.~\ref{histograms}C).
%   \end{spacing}
% \end{fullbox}

\clearpage

  \begin{figure}[h!]
      \caption{Despite the high levels of replicaiton in our example dataset, most galler-natural enemy pairs were never observed co-occurring, and those that did co-occur were rarely observed interacting. \textbf{A)} Here we show a histogram of the number of pairs of species observed co-occurring at least once; the 9794 galler-enemy pairs that were never observed co-occurring are omitted.
      \textbf{B)} Here we show a histogram of the number of observed interactions within pairs of co-occurring species. Species which co-occurred but never interacted are included. 
      \textbf{C)} Here we show, for each species pair, the number of observed interactions plotted against the number of observed co-occurrences. The red, dashed line indicates a 1:1 relationship between interactions and co-occurrences.}
      \label{histograms}
      \begin{center}
      \includegraphics*[height=.6\textheight]{figures/GP_histogram.eps}
      \end{center}
      \end{figure}


  \begin{figure}[h!]
    \caption{Imperfect detection of interactions increases the number of samples required to be confident that an interaction does not occur. Assume that we want to infer the probability of an interaction between two species, $i$ and $j$, and that this interaction is completely impossible (i.e. the true $\lambda=0$). When estimating $\lambda$, the number of observed interactions will follow a binomial distribution with number of interactions $k$ and number of observations $n$. Using this distribution, we can compute the credible interval of the estimated probability $\lambda$. A single observation of species co-occurrence reveals very little regarding the probability of the interaction as the credible interval for a pair of species with one observation essentially spans from 0 to 1. Only with 35 observations will the upper limit of the credible interval be lowered to 0.1 (dashed, red line). Thus, adding more observations is useful in controlling uncertainty, but the number of observations added needs to be very high. Here we show the upper bound (solid black line) of a 95\% Clopper-Pearson true credible interval for $\lambda$ when $k=0$ ($i$ and $j$ have not been observed interacting) for a variety of $n$. A threshold interaction probability of 0.1 is indicated by the dashed red line. }
    \label{upper_limits}
    \begin{center}
    \includegraphics*[width=.8\textwidth]{figures/upper_limit_DG.eps}
    \end{center}
    \end{figure}


  \begin{figure}[h!]
    \caption{Using prior distributions based on the galler-natural enemy network sampled at a single site in~\citet{Kopelke2017}, we calculate posterior distributions for the probability of interaction ($\lambda$) between two species that have not yet been observed interacting ($k = 0$). 
    Posterior distributions for $\lambda$ narrow and approach zero as the number of observed co-occurrences ($n$) increases. Likewise, the maximum likelihood estimator for the mean probability of interaction (diamonds at top of each panel) approaches zero and the 95\% credible interval (lines at top of each panel) narrows as sample size increases.
    Dashed lines indicate threshold probabilities of 0.01, 0.05, and 0.1. The number of samples required to obtain a 95\% credible interval below each threshold increases rapidly. It takes just over 20 observed co-occurrences to be 95\% confident that $\lambda<0.10$, approximately 50 co-occurrences to be 95\% confident that $\lambda<0.05$, and over 100 co-occurrences to be 95\% confident that $\lambda<0.01$.}
    \label{Salix_pdfs_cdfs}
    \begin{center}
    \includegraphics*[width=.8\textwidth]{figures/GP_pdfs_increasing_N_Zillis.eps}
    \end{center}
    \end{figure}

  % \begin{figure}[h!]
  %   \caption{The number of samples required to achieve a given level of confidence that an interaction probability $\lambda_{ij}$ is below a given threshold varies with both parameters. With a low threshold, our confidence that $\lambda_{ij}$ is below the threshold increases rapidly with repeated observation of co-occurrence without interaction. Here we show the cumulative density functions for threshold probabilities of 0.5 (solid line), 0.25 (dashed line), 0.1 (dash-dot line), and 0.05 (dotted line) as well as the points at which the cdf reaches 0.90 (orange square), 0.95 (red circle), and 0.975 (blue diamond) for each threshold value. The large ticks along the x-axis indicate the number of samples associated with each of these points. \textbf{A)} In the \emph{Salix}-galler network component, the 95\% credible interval for $\lambda_{ij}$ when $n$=0 was (0.013, 0.049). We can therefore be at least 95\% confident that $\lambda_{ij}$ is below thresholds of 0.1 or 0.05 without any observed co-occurrence of species $i$ and $j$. To be confident that $\lambda_{ij}$ is less than 0.01, however, would require more observed co-occurrences than there are sites in our dataset. \textbf{B)} In the galler-parasitoid network component, the 95\% credible interval for $\lambda_{ij}$ was substantially broader and many observed co-occurrences ($\approx$ 15-35) are required to be 95\% confident that $\lambda_{ij}$ is below thresholds of 0.1 or 0.05.}
  %   \label{Salix_cdfs}
  %   \begin{center}
  %   \includegraphics[width=.5\textwidth]{figures/Salix_Galler_samples_and_cdfs_Zillis.eps}
  %   \end{center}

  %   \end{figure}


  \begin{figure}[h!]
    \caption{Mean connectance, links per galler, nestedness (NODF), and links per natural enemy for networks assembled using posterior distributions based on a single sub-network in the~\citet{Kopelke2017} dataset (Zillis) occupied a narrow range. We created 100 ``posterior-sampling" networks and then, for each of these, created 100 ``detection-filter" networks by randomly sampling 50\%-99\% of the interactions included in the posterior-sampling network. This simulates imperfect detection of interactions in the field. Each point represents the mean (+/- SD) network property (e.g., connectance) obtained from a set of 100 detection-filter networks. For each property and both network types, the detection-filter networks cover a broader range of network properties than the posterior-sampling networks. The value of each property decreases with the proportion of links included in the detection-filter networks; values in the original web are indicated by dashed lines.}
    \label{posterior_webs}    
    \begin{center}
    \includegraphics[width=.8\textwidth]{figures/GP_posterior_properties_Zillis.eps}
    \end{center}
    \end{figure}


\clearpage

    \bibliographystyle{ecol_let} 
    \bibliography{manual_abbrev} % Abbreviate journal titles.


\end{document}



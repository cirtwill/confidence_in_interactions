\documentclass[12pt]{article}
\usepackage{amsmath} 
\usepackage[dvips]{graphicx}
\usepackage{multirow} 
\usepackage{geometry} 
\usepackage{pdflscape}
\usepackage[labelfont=bf]{caption} 
\usepackage{setspace}
\usepackage[running]{lineno} 
% \usepackage[numbers,sort]{natbib}
\usepackage[round]{natbib} 
\usepackage{array}
\usepackage{hyperref,url}
\usepackage{float}
\usepackage{mdframed}
\makeatletter


% this creates a custom and simpler ruled box style
\newcommand\floatc@simplerule[2]{{\@fs@cfont #1 #2}\par}
\newcommand\fs@simplerule{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@simplerule
  \def\@fs@pre{\hrule height.8pt depth0pt \kern4pt}%
  \def\@fs@post{\kern4pt\hrule height.8pt depth0pt \kern4pt \relax}%
  \def\@fs@mid{\kern8pt}%
  \let\@fs@iftopcapt\iftrue}

% this code block defines the new and custom mdframed environment
\newmdenv[rightline=true,bottomline=true,topline=true,leftline=true,linewidth=2pt]{fullbox}


\newcommand{\methods}{\textit{Materials \& Methods}}
\newcommand{\SI}{\textit{Appendix}~}

\topmargin -1.5cm % 0.0cm 
\oddsidemargin 0.0cm % 0.2cm 
\textwidth 6.5in
\textheight 9.0in % 21cm
\footskip 1.0cm % 1.0cm

\usepackage{authblk}

\title{A quantitative framework for investigating the reliability of empirical network construction}


\author{Alyssa R. Cirtwill$^{1\dagger}$, \&  Anna Ekl\"{o}f$^{1}$, Tomas Roslin$^{2}$, Kate Wootton$^{2}$, Dominique Gravel$^{3}$}
\date{
% \begin{minipage}[h]{0.6\textwidth}
\small$^1$ Department of Physics,\\
Chemistry and Biology (IFM)\\ 
Link\"{o}ping University\\
Link\"{o}ping, Sweden\\
\medskip
\small$^2$ Department of Ecology\\ 
P.O. Box 7044\\ 
Swedish University of Agricultural Sciences \\ 
SE-750 07 Uppsala, Sweden\\
\medskip
\small$^3$ D\'{e}partement de biologie\\ 
Universit\'{e} de Sherbrooke\\ 
Sherbrooke, Canada
\medskip
\small$^\dagger$ Corresponding author:\\
alyssa.cirtwill@gmail.com\\
\medskip
\medskip
\normalsize Running head: Constructing quantitative interaction networks
% tel: +46 723 158464\\
% $^\ddagger$ anna.eklof@liu.se\\
% $^\star$ tomas.roslin@slu.se\\
% $^\diamond$ kate.wootton@slu.se\\
% $^\triangleright$ dominique.gravel@usherbrooke.ca
}


\renewcommand\Authands{ and }

\begin{document} 
\maketitle 
\raggedright
\setlength{\parindent}{15pt} 

\vspace{-.4in}

% {\small

% \section*{\small Details}

% \begin{minipage}[h]{0.6\textwidth}
% \begin{itemize}
% \item Running title: Quantitative network construction
% \item Number of references: 37
% \item Number of figs, tables, \& text boxes: 6 % Limit is 10
% \end{itemize}
% \end{minipage}\begin{minipage}[h]{0.4\textwidth}
% \begin{itemize}
% \item Abstract word count: 290
% \item Main text word count: 6872 % Limit is 6000-7000 including figs, tables, references. 
% \end{itemize}
% \end{minipage}
% }

\newpage

\begin{spacing}{2.0}

\section*{Abstract}
  [[was fine? Maybe lightly edit to make clear we're not doing a review]]
  \begin{enumerate}

    \item  Descriptions of ecological networks typically assume that the same interspecific interactions occur each time a community is observed. This contrasts with the known stochasticity of ecological communities: community composition, species abundances, and link structure all vary in space and time. Moreover, finite sampling generates variation in the set of interactions actually observed. For interactions that have not been observed, most data sets will not contain enough information for the ecologist to be confident that unobserved interactions truly did not occur.
    \item Here we develop the conceptual and analytical tools needed to capture uncertainty in the estimation of pairwise interactions. To define the problem, we identify the different contributions to the uncertainty of an interaction. We then outline a framework to quantify the uncertainty around each interaction by combining data on observed co-occurrences with prior knowledge. We illustrate this framework using perhaps the most extensively sampled network to date. 
    \item We found significant uncertainty in estimates for the probability of most pairwise interactions. This uncertainty can, however, be constrained with informative priors. This uncertainty scaled up to summary measures of network structure such as connectance or nestedness. By using a comprehensively sampled network as a test case, we find that even with informative priors we are likely to miss many interactions that may occur rarely or under different local conditions. 
    \item Overall, we demonstrate the utility of our approach, given the importance of acknowledging the uncertainty inherent in network studies, and the utility of treating interactions as probabilities in pinpointing areas where more study is needed. Most importantly, we stress that networks are best thought of as systems constructed from random variables, the stochastic nature of which must be acknowledged for an accurate representation. Doing so will fundamentally change networks analyses and yield greater realism.
  % Can be up to 350 words for MEE.
\end{enumerate}


\section*{\small Keywords}

ecological networks; probabilistic interactions; Bayesian networks; sampling error; spatial variability; temporal variability; uncertainty

\linenumbers
\clearpage

\section*{Introduction}

    Representing an ecological community as a network allows ecologists to address both community composition and the interactions between species~\citep{Roslin2016}. Identifying these interactions is a crucial step towards understanding (and predicting) the effects that species have on each other and the response of the community as a whole to external perturbations~\citep{Bartomeus2016,Giron2018}. Importantly, information is conveyed both by the presence and \emph{absence} of links between species. Presences and absences are not, however, equally certain. An observed link definitely occurred, but there are multiple reasons why a given link may not be observed~\citep{Jordano2016}. 


    First and foremost, ecological communities are known to be stochastic~\citep{Gotelli2000}. Community composition and abundances vary over space~\citep{Baiser2012} and time~\citep{Olesen2011a}, leading to variation in encounter probabilites and hence in interactions~\citep{Vazquez2005,Poisot2015,Graham2018}. Species must co-occur in order to be observed interacting, so any variation in co-occurrence will create variation in the set of observed interactions~\citep{Gravel2013,Graham2018}. Even assuming constant co-occurrence within a given study system, changing abundances can affect interactions if species which would otherwise interact become too rare to detect each other~\citep{Tylianakis2010,Jordano2016}. Moreover, interactions vary over space~\citep{Kitching1987,Baiser2012,Emer2018}, time~\citep{Kitching1987,Olesen2011a,Lopez2017}, between individuals~\citep{Pires2011a,Wells2013,Fodrie2015,Novak2015}, throughout life cycles~\citep{Preston2014,Clegg2018}, and with environmental conditions~\citep{Poisot2015}.


    Beyond ``true" variation in network structure, several researchers have noted the effects of sampling intensity (e.g.,~\citealp{Martinez1999,Bluthgen2006,Bluthgen2007,Jordano2016}). An assessment of the accumulation of interactions with increasing sampling effort suggests that it is even more challenging to correctly document interactions than species~\citep{Guimera2009,Poisot2012,Bartomeus2013,Jordano2016,Giron2018,Graham2018}. As a result, it has been proposed that interactions should be described probabilistically and network metrics computed accordingly~\citep{Bartomeus2013,Poisot2016}. Efforts in this vein usually take the form of creating model food webs that approximate observed empirical networks~\citep{Allesina2008,Guimera2009,Williams2010,Rohr2016}. These models may be based on species traits and/or abundances~\citep{Rohr2016,Weinstein2017,Weinstein2017a,Graham2018} or on simple abstract rules~\citep{Allesina2008,Guimera2009,Williams2010}. Such models provide an important ''reality check'' by allowing us to test hypotheses about the factors structuring ecological networks~\citep{Bartomeus2013,Weinstein2017,Weinstein2017a,Graham2018}. They can also allow us to estimate the numbers of interactions which have not been observed~\citep{Jordano2016,Weinstein2017a} and predict which unobserved interactions are most likely~\citep{Guimera2009,Bartomeus2013}. 


    [[These two paragraphs try to frame our study more clearly]]
    To date, models incorporating uncertainty have either been very general (e.g.,~\citet{Guimera2009,Gravel2013}) or very system-specific (e.g.,~\citet{Bartomeus2013,Weinstein2017,Weinstein2017a,Graham2018}); in either case one of these models may be a poor fit for a particular study system. Moreover, some of the system-specific models described above cannot accommodate species observed only once~\citep{Bartomeus2013,Weinstein2017}. As rare species are likely to be observed less frequently~\citep{Bluthgen2006} and may be of particular interest (e.g., when modelling species loss), there is a need for a more widely-applicable framework that includes such rare species. 


    Here we introduce a simple Bayesian framework that can easily be adapted to different systems. First, we provide a brief description of the nested levels of uncertainty affecting ecological network construction. In the context of this uncertainty, which can be reduced but not eliminated by high-quality sampling (\citealp[see Box 1]{Bartomeus2013}), we present our Bayesian framework for combining observed data with a prior expectation of network structure. We conclude with a worked example applying this framework to an intensively-sampled host-parasitoid network. We find that even the highest-quality empirical network we could find had substantial uncertainty about whether many pairs of species interact, demonstrating the need to include information about this uncertainty in published networks. We also show that this interaction-level uncertainty leads to uncertainty about estimates of network structure, with important implications for comparisons across networks; especially those compiled with different sampling intensity~\citep{Weinstein2017}.


    % Development of molecular sampling methods such as DNA barcoding hold great potential for uncovering interactions missed by other sampling methods (such as visit surveys)~\citep{Giron2018}, but these novel methods are also somewhat error-prone at present~\citep{}. 

    % Accounting for differences in feasible (binary) interactions across life stages changes network structure~\citep{Clegg2018}.

    % Representing an ecological community as a network summarises both species composition and interactions between species. A tabulation of the nodes (species) and their relative abundances forms the basis for traditional metrics of community composition such as alpha diversity. A network framework combines this tabulation of nodes with a list of interactions (links between nodes) so that networks provide additional, higher-order information on community structure. Despite this additional information, empirical descriptions of ecological networks are still often limited by a lack of data or tools to adequately incorporate variation in interactions into networks. That is, whether the network is assembled based on aggregated data, a single intensive ``snapshot" sample, or expert knowledge, interactions are assumed to occur deterministically wherever and whenever the community is observed~\citep{Olesen2011a}. 


    % The assumption of static communities contrasts with the known stochasticity of ecological communities~\citep{Gotelli2000}. Community composition and abundances vary over space~\citep{Baiser2012} and time~\citep{Olesen2011a}. Moreover, interactions vary over space~\citep{Kitching1987,Baiser2012}, time~\citep{Kitching1987,Olesen2011a}, and between individuals~\citep{Pires2011a,Fodrie2015,Novak2015}. Worse, variability in community composition and interactions may not be closely related. The removal of a species from a site will obviously also remove its interactions but, conversely, the co-occurrence of potentially interacting species does not guarantee that the species will interact at a given place and time. Interactions can be lost if the interaction partners remain present but are separated in time or are too rare to detect each other~\citep{Tylianakis2010}. Interactions can also fail to occur because of environmental contingencies~\citep{Poisot2015} or individual preferences~\citep{Fodrie2015}. 


    % Beyond ``true" variation in network structure, several researchers have noted the effects of sampling intensity (e.g.,~\citealp{Martinez1999,Bluthgen2006,Bluthgen2007}). An assessment of the accumulation of interactions with increasing sampling effort suggests that it is even more challenging to document interactions than species~\citep{Poisot2012,Bartomeus,others}. As a result, it has been proposed that interactions should be described probabilistically and network metrics computed accordingly~\citep{Poisot2016}. Early work in this vein includes food-web models using likelihood-based approaches~\citep{Allesina2008} or Gaussian~\citep{Williams2010} or binomial~\citep{Rohr2016} probability functions for each possible interaction. These models may include information about species' traits~\citep{Rohr2016} or attempt to reproduce empirical network structures using a set of simple rules~\citep{Allesina2008,Williams2010}.


    % Despite these efforts, we currently lack the quantitative methodology to deal with uncertainty generated by spatiotemporal variation in ecological interactions and by limited sampling. [[cite previous attempts]] Even in well-sampled networks, uneven sampling across species can lead to the inference that some species do not interact because they co-occur rarely or have not yet been observed together - even if they do interact when they do co-occur (see Fig.~\ref{histograms}). Nearly all network studies will thus neglect some interactions, necessitating an approach that acknowledges this uncertainty.


    % In this study, we formalise the description of interactions between species as probabilities and develop analytical tools to capture the uncertainty in the estimation of these interactions. We focus on binary interactions as a first step, but the framework could be expanded to deal with interaction frequencies or strengths. To define the problem, we first identify different contributions to the uncertainty of an interaction and discuss the implications of each source of uncertainty for the properties of ecological networks. 


    \subsection*{Why do some interactions \emph{not} occur?}

      We start from the perspective of a community ecologist faced with the task of describing a previously unknown interaction network. This ecologist will be interested in generating a description of the species/nodes present and the links between them~\citep{Roslin2016}.  Importantly, the information sought is conveyed by both the presence and \emph{absence} of links. Presences and absences are not, however, equally certain. An observed link definitely occurred (assuming we can correctly identify the species involved), but there are multiple reasons why a given link may not be observed \emph{whether or not the interaction truly occurred}. Moreover, given an unobserved interaction in an empirical dataset, we often cannot determine why the interaction did not occur \emph{post hoc}. The detection of any interaction is a stochastic process subject to many levels of uncertainty. As a conceptual guide to the factors affecting this process, we describe three nested levels of uncertainty that roughly address the questions: "Could species $i$ and $j$ interact?", "Did they interact?" and "Did we observe the interaction?" (Fig.~\ref{conceptual_fig}). 


        \subsubsection*{Could the species interact?} 

          First, and most fundamentally, some interactions are feasible if a pair of species should co-occur while others are not~\citep{Poisot2015}. Assuming that the feasibility of interactions depends on the traits of species involved, we can define the probability of an interaction $L_{ij}$ between species $i$ and $j$, given some description of trait-matching $\mathbf{T_{ij}}$, as $P(L_{ij}|\mathbf{T_{ij}}) = \lambda_{ij}$~\citep{Bartomeus2013,Gravel2013,Weinstein2017}. 
          In some cases the traits prohibiting an interaction are known, and there will be little uncertainty about $\lambda_{ij}$.
          In most cases, however, it is not known what traits might prohibit an interaction~\citep{Dormann2017}. Pollinators may visit plants outside of those predicted based on "pollination syndromes"~\citep{Weinstein2017a} and herbivores may consume animal prey when it becomes accessible~\citep{Furness1988,Pietz2000}. These examples illustrate the danger in assuming that seemingly unlikely interactions cannot occur.


          An unobserved interaction may be a rare phenomenon with $\lambda>0$ and the associated zero in an interaction matrix might be false. This uncertainty is partly addressed by trait-matching models in which species with better-matched traits are more likely to interact~\citep{Bartomeus2016,Jordano2016,Weinstein2017}. As these models improve and include better information about the influence of traits on interactions, the uncertainty will be reduced~\citep{Jordano2016} and $\lambda_{ij}$ should either tend to 0 or to 1. Nevertheless, every model is imperfect and lacks information that could be used to define constraints on at least some interactions~\citep{Dormann2017}. Until these gaps are filled, some uncertainty will remain about whether some species pairs could interact.


        \subsubsection*{Did the species interact during sampling?} 

          Assuming that an interaction is feasible (i.e., $L_{ij} = 1$;), an interaction may still not occur at a particular place or time~\citep{Poisot2015,Graham2018}. This could be because constraints such as inclement weather or low abundance prevented the interaction~\citep{Jordano2016,Graham2018}, a more preferred interaction partner was available~\citep{Weinstein2017a}, or substantial individual variation in traits meant that local individuals could not interact even if the species in general can~\citep{Gravel2013,Wells2013,Poisot2015}. For example, a rare galler may not parasitize a rare plant because they do not encounter each other. In metawebs describing a community found at several sites, an interaction may also fail to occur because two species do not co-occur at a particular site even if they do co-occur and interact elsewhere~\citep{Graham2018}. We can define the local realisation of an interaction, $X_{ij}$, given that the interaction is feasible, as a stochastic process with associated probability $P(X_{ij}|L_{ij} = 1) = \chi_{ij}$. It is unlikely that $\chi_{ij} = 1$  except in the case of obligate specialists who always interact whenever they co-occur and always co-occur. When $\chi_{ij} < 1$ we cannot be sure whether an unobserved interaction cannot occur or whether it simply did not occur at a particular site.


          Some of the uncertainty about $\chi_{ij}$$ could be addressed by drawing on the rich literature about interaction contingencies found in community ecology. Phenological matching~\citep{MillerRushing2010,Gezon2016}, species preferences~\citep{Pires2011,Novak2015,Coux2016}, and fear effects~\citep{Luttbeg2005,Wirsing2008} all affect the probability of an interaction occurring at a particular site and time, and could be included in models similar to trait-matching models. Alternatively, interaction probabilities could be tested in mesocosm studies with constant conditions. Some studies attempt to reduce uncertainty by sampling intensively over a short period of time and restricting the scale of interest to interactions occurring at the sampling site and time (conflating $\chi_{ij}$ and $\lambda_{ij}$)~\citep{Bartomeus2013,Weinstein2017,Weinstein2017a}. This approach may be suitable for some research questions, but is not appropriate if we wish to assemble the full set of interactions which occur in a community (beyond the specific site and time of sampling). In contrast, expanding sampling to cover a broader spatial or temporal range (e.g., sampling in a variety of microhabitats or during a variety of weather conditions) will help to reduce uncertainty about $\chi_{ij}$ by including a broader range of parameters affecting interaction probability. It is unlikely, however, that resources will permit researchers to sample in the full range of sites and conditions covered by a community, and some uncertainty about $\chi_{ij}$ will remain.

         % One proposed solution to the problem of process uncertainty is to use repeated sampling of networks. For example,~\citet{Weinstein2017-wrong one?} used repeated sampling to estimate the daily probability of detecting an interaction and thereby attempted to separate detection and process uncertainty.[[This apparently confused R3]] While conceptually attractive, this approach is unsuitable for interactions occurring over longer time scales (e.g., associations between hosts and parasitoids with a single generation per year), which would require multi-year sampling efforts, or very rare interactions which would require extremely high sampling effort to detect. Moreover, the faster accumulation of species than interactions means that repeated sampling will tend to introduce more unobserved interactions than observed interactions unless the number of samples is high enough to ensure that all species present have been observed~\citep{Poisot2012,Jordano2016}.


          \textbf{Forbidden links}

            Both whether species can interact and whether they do interact at a particular place and time have been discussed in the context of ``forbidden links"~\citep{Jordano2016}. In this framework, all links which are prevented by spatio-temporal uncoupling, physiological constraints, etc. are considered "structural zeros" that cannot be observed~\citep{Jordano1987,Jordano2016}. We conceptually distinguish between physiological constraints and spatio-temporal uncoupling to allow for cases in which a species is introduced to a new habitat, expands its range, or shifts phenology~\citep{Gravel2013}. In such cases, links which are forbidden due to physiological constraints remain forbidden but links previously forbidden due to spatio-temporal mismatch could potentially occur. 


        \subsubsection*{Did we observe all interactions that occurred?} 

          Lastly, measurement errors are a pervasive source of uncertainty in the observation of ecological processes. Even if an interaction is feasible and occurs during sampling ($L_{ij} = 1$ and $X_{ij} = 1$), we may still fail to detect it~\citep{Jordano2016,Weinstein2017}. We may define the probability of detecting of an interaction, $D_{ij}$, given the interaction is feasible and occurs, as a stochastic process with the associated probability $P(D_{ij}|X_{ij} = 1,L_{ij} = 1)=\delta_{ij}$. Detection failure could happen for many reasons (see~\citet{Wirta2014} for examples and partial solutions to them). Some sources of detection error can be minimised with appropriate sampling effort (e.g., combining plant-focused and animal-focused sampling when assembling pollination networks~\citet{Jordano2016}), but other sources are often difficult to reduce. For example, cryptic species might require molecular analysis for appropriate taxonomic identification~\citep{Wirta2014,Frost2016} or it may not be possible to sample some species due to conservation concerns~\citep{Lagrue2015}. 


          Some types of interactions will have higher detection uncertainty than others. Pollination, for example, occurs briefly and does not leave long-lasting evidence, while parasitism can be a lifelong association between species. Even long-term interactions can be missed, however, especially if species are difficult to identify or if not all individuals of a species share the interaction~\citep{Wells2013,Cirtwill2016}. Parasites, for example, are often concentrated in only a few individuals~\citep{Lagrue2015}. If these infected individuals do not happen to be included in a sample, their interactions will be missed. Detection probabilities also vary widely between species, and potentially individuals within species, due to different abundances and behaviours~\citep{Wells2013,Weinstein2017}.


    \subsection*{Can't we just sample more?}

        To sum up, the absence of an interaction on a given day could  be because it was not feasible [$P(L_{ij}) = 0$], was feasible but did not occur because of local conditions during sampling ($P(X_{ij}|L_{ij} = 1) = 0$], or was feasible and occurred during sampling but could not be observed [$P(D_{ij}|X_{ij} = 1,L_{ij} = 1) = 0$]. Note that some exteral factors can affect more than one level of uncertainty. For example, rare species are less likely to interact at a given site due to neutral processes~\citep{Jordano2016,Graham2018}. Interactions involving rare species are also less likely to be detected, unless more sampling effort is focused on rare than common species~\citep{Bartomeus2013,Jordano2016}. Thus, abundance affects both the probability of an interaction occurring at a particular site \emph{and} our ability to observe the interaction. 


        When considering a metaweb, we wish to separate the unobserved interactions where $L_{ij} = 0$ (i.e., unfeasible interactions) from interactions which could occur but did not at a particular sampling site or which occurred but were not detected (Fig.~\ref{conceptual_fig}). Unfortunately, this is not possible for most interactions (except those where we know an interaction is truly not feasible). That is, we generally do not know \emph{why} an interaction was not observed, only that it was not. An empirical ecologist will instead measure the marginal probability $P(L_{ij}) = k_{ij}/n_{ij}$, where $k_{ij}$ is the number of observed interactions between species $i$ and $j$ and $n_{ij}$ the number of observed co-occurrences. Given this information, the question becomes: how can we reduce the uncertainty around our estimated interaction probability?


        The obvious rule of thumb to reduce uncertainty is ``sample more". Where resources permit, increasing sample sizes will reduce uncertainty about the upper bound of the interaction probability and will also increase the probability of detecting unlikely or cryptic interactions (e.g., interactions where $L_{ij} = 1$ but process or detection uncertainty is high). Despite these benefits, we note that there are limits to the utility of increased sampling. Since the probability of observing the co-occurrence of two species will always be higher than the probability of observing their interaction (since the probability of interaction is conditional on both interaction partners being present), we will accumulate observations of co-occurrences faster than we will accumulate observations of interactions (Fig.~\ref{histograms}C). While this improves our understanding of the set of species present in the community, it introduces yet more uncertainty into the interaction matrix. Note that it is better to identify the set of potential interactions than to miss a species, so sampling effort should not be reduced in order to exclude rare species.


        An added complication is that increased sampling will not reduce uncertainty evenly across interactions. To record an interaction between species $i$ and $j$, we need to identify both partners correctly (a non-trivial problem in many food webs; e.g.~\citealp{Kaartinen2011,Roslin2016}). For both molecular and rearing techniques, certain types of interactions may go unnoticed due to technical challenges~\citep{Wirta2014}. This can bias the set of recorded interactions towards those that are easier to observe~\citep{Carstensen2014,Jordano2016}. Combining multiple sampling types can reduce this type of bias~\citep{Wirta2014,Jordano2016}. Nevertheless, despite substantial improvements in recent decades, most available interaction networks are still undersampled and, moreover, are only subsets of larger communities~\citep{Bartomeus2013,Jordano2016}. 


        In short, high-quality sampling is an essential component of ecological network construction and increasing sampling (length of time, number of sites, number of distinct methods, etc.) can reduce some of the uncertainty about unobserved interactions. We believe, however, that empirical researchers generally sample to the maximum extent possible given constraints of time, resources, and study sites. This practical constraint means that reducing one source of uncertainty is likely to increase another source (e.g., sampling intensively at a single site may reduce detection uncertainty but will increase uncertainty due to variation between sites). The number of observed co-occurrences needed to believe that an unobserved interaction cannot occur is very high (about 35; Fig.~\ref{upper_limits}), and most datasets will not include this level of sampling for all pairs of species~\citep{Bartomeus2013}. For pairs of species with fewer observations, we can reduce uncertainty by combining the observed data available with prior knowledge about the system. This is straightforward to do using a Bayesian approach, as detailed below.


\section*{Materials and methods}

  \subsection*{A Bayesian framework for interaction probabilities}

    % Maybe we can move much of this to the Box and satisfy reviewer 1


    % We are most interested in how to quantify an interaction probability, and its associated uncertainty, for interactions that have not been observed. For interactions which have been observed, methods already exist to quantify the probability of the interaction. 


    % Assume that a pair of species has been observed co-occurring $n$ times and interacting in $k = 0$ cases. We want to know the probability $\lambda_{ij}$ that the species would be observed interacting if not constrained by local conditions, imperfect detection, etc. We can consider the occurrence of an interaction as a Bernoulli trail where the number of successes ($k$) over $n$ trials will follow a binomial distribution (see~\emph{Appendix S2} for a full description of the mathematical framework). In this framework, the maximum likelihood estimate (MLE) for $\lambda_{ij}$ is:

    %     \begin{equation}
    %       \lambda_{MLE} = \frac{k}{n}  .
    %       \label{theta_MLE}
    %     \end{equation}
  

    % Note that $\lambda_{ij}$ is not a point estimate but rather a random variable with an unknown distribution. This means that if $k = 0$ in a given sample, this does not necessarily imply that the two species will never interact. Rather, $k = 0$ implies that `no interaction' is the most likely outcome when the species do co-occur but there is nonetheless some chance that the two species \emph{could} interact. In the situation where $k>0$, in contrast, we are sure that the interaction is feasible ($L = 1$). If $n>k>0$, the interaction is feasible but there may be local constraints ($X_{ij} = 0$) or detection errors ($D<1$) causing the interaction not to be observed in some samples. 


    % To properly interpret $\lambda_{ij}$, we need to estimate the variance as well as the MLE. The variance of a Bernoulli experiment is $n\lambda$(1-$\lambda_{ij}$) and, importantly, describes the variability of the number of successes $k$ for $n$ trials rather than the variance associated with the estimation of $\lambda_{ij}$. It is, however, straightforward to compute the confidence interval for the MLE of $\lambda_{ij}$ using any of several methods, including the \emph{Wilson score interval}, the \emph{Clopper-Pearson interval}, and the \emph{Agresti-Coull interval} (for details, see [\citealp{Brown2001}]). 


    % All of the above methods for estimating the variance of $\lambda_{ij}$ include the number of samples $n$. This means that where the number of samples $n$ is very low (e.g., for rare species), there will be considerable uncertainty around our estimate of $\lambda_{ij}$. In Fig.~\ref{upper_limits}, we derive the Clopper-Pearson interval to explore how the estimate of $\lambda_{ij}$ (for true $\lambda = 0$) varies with sample size. At a small sample size, the 95\% confidence interval is nearly (0, 1). To establish that species are not interacting with any acceptable certainty requires tens of observations of the two species co-occurring but not interacting. Most data sets will lack such extensive sampling across all species pairs, but we can still use a Bayesian approach to supplement the available data with other sources of information.

    The basis of a Bayesian approach to modelling the probability that an interaction between species $i$ and $j$ occurs ($\lambda_{ij}$) is combining the maximum likelihood estimate (MLE) of $\lambda_{ij}$ with a prior distribution (described in the next section) and a normalising function. The MLE of $\lambda_{ij}$ can be modelled as a Bernoulli trial based on the number of observed interactions $k_ij$ and observed co-occurrances $n_{ij}$: $\lambda_{ij} = \frac{k_{ij}}{n_{ij}}$. The most appropriate prior distribution for a probability such as $\lambda_{ij}$ is the beta distribution:

    \begin{equation}
          \lambda_{ij} \sim Beta(\alpha_{ij},\beta_{ij}) , \label{prior}
        \end{equation}

        \noindent which has two shape parameters, $\alpha_{ij}$ and $\beta_{ij}$. 

    The shape parameters, or hyperparameters, may be set to particular values or derived from some data (see below). Note that the hyperparameters may be set to the same values for all interactions, allowed to vary independently for all interactions, or may incorporate non-independence betwween interactions (e.g., when the shape of the prior distribution depends upon species' abundances or traits).
    Combining the prior distribution with the MLE of $\lambda_{ij}$ derived from observed data, we obtain a posterior distribution that also follows a beta distribution with new parameters $\alpha_{ij}'= \alpha_{ij}+k$ and $\beta_{ij}'=\beta_{ij}+n-k$ (see~\emph{Appendix S2} for details). The influence of the prior on the posterior distribution can be understood from these parameter definitions: the difference between the posterior and the prior will increase with $k_{ij}$ and $n_{ij}-k_{ij}$. In other words, the distribution of $\lambda_{ij}$ for better-sampled pairs of species will rely less on the information used to build the prior distribution and depend more on the observed data and uncertainty about the probability of the pair interacting will decrease (Fig.~\ref{Salix_pdfs_cdfs}). Note that these Bayesian models of $\lambda_{ij}$ are designed to quantify (and reduce) the total uncertainty about $\lambda_{ij}$. They do not address any of the nested sources of uncertainty in particular except through the choice of prior.

    % R1 suggests removing this - could be in the SI I guess?
    % \subsubsection*{Moments and other properties}

    %   It is common to perform analyses that require calculating higher-order network properties in interaction networks. The fact that the posterior distribution of $\lambda_{ij}$ follows a beta distribution makes it straightforward to compute moments and other properties needed for these analyses. 


    %   The \textbf{average} of $\lambda_{ij}$ is: 
    %       \begin{equation}
    %         \bar{\lambda} = \frac{\alpha_{ij}+k}{\alpha_{ij}+\beta_{ij}+n} ,
    %         \label{mean}
    %       \end{equation}

    %     and its \textbf{variance} is:  
    %       \begin{equation}
    %         Var(\lambda|k) = \frac{(\alpha_{ij} + k)(\beta_{ij} + n - k)}{(\alpha_{ij} + \beta_{ij} + n)^{2}(\alpha_{ij} + \beta_{ij} + n +1)}
    %         \label{variance}
    %       \end{equation}

    %     The \textbf{mode} of the distribution is:
    %       \begin{equation}
    %         \hat{\lambda} = \frac{\alpha_{ij} + k - 1}{\alpha_{ij} + \beta_{ij} + n - 2} .
    %         \label{mode}
    %       \end{equation}


    \subsubsection*{Choosing a prior distribution}  

      The choice of prior distribution will reflect the goal of a study, the amount of prior knowledge available, and the strength of researchers' belief in that prior knowledge. Note that there is no 'correct' prior as long as we do not know the true distribution of interaction probabilities~\citep{Spiegelhalter2000}. The goal is to select a reasonable prior that reflects our assumptions about the true distribution, and then adjust this prior with observed data as it becomes available. When in doubt, it is also possible to model interaction probabilities using a variety of priors and then compare results~\citep{Spiegelhalter2000}. Some researchers may be reluctant to adopt a Bayesian approach because of its perceived subjectivity (i.e., the metaweb we obtain will depend upon the assumptions used when developing a prior). We argue that one of the advantages of the Bayesian approach is that it makes some of the subjectivity inherent in network construction explicit. Researchers compiling empirical networks face many decisions about where to set community boundaries, the set of species to include in the focal community~\citep{Jordano2016}, what type of sampling is most appropriate~\citep{Wirta2014}, etc. All of these choices will affect the compiled network and most do not have clear answers, making all empirical networks (and all theoretical networks, which likewise depend upon the assumptions used in building the network) to some extent subjective. In addition, researchers distinguishing ``forbidden links" inherently rely on prior knowledge to identify these links~\citep{Jordano2016}. Stating that an interaction is ``forbidden" is, in effect, applying a very strong prior to reduce/eliminate uncertainty about the non-observation of that interaction. Given this reality, we feel it is better to make the assumptions used to build a network as explicit as possible so that these assumptions can be questioned, compared between studies, and tested. 


      \textbf{Uninformative priors}

        If researchers wish to reflect uncertainty in their data without applying any particular assumptions/prior knowledge or account for the possibility that the distribution of probabilities of interaction may vary, they can select $\alpha_{ij}$ and $\beta_{ij}$ to produce an intentionally uninformative prior~\citep{Leyland2005,Berger2006}. This prior can then be given a low weight, such that any observed data will outweigh the prior distribution. For pairs of species with little or no observed data, an uninformative prior will leave large variance about $\lambda_{ij}$ and reflect our lack of knowledge about the pair. Since we model $\lambda_{ij}$ as a Bernoulli trial, the appropriate uninformative prior is Jeffreys prior: a beta distribution with $\alpha_{ij}$=$\beta_{ij}$=0.5. The sum of $\alpha_{ij}$ and $\beta_{ij}$ is roughly equivalent to the weight of the prior, so Jeffreys prior has equal weight to a single observation. 


      \textbf{Informative priors}

        In most cases, we have at least some prior knowledge about a system which we can use to reduce uncertainty about interactions. One common example is the well-justified assumption that plants will not consume animals in a food web; excluding carnivorous plants it is not contentious to say that we have little to no uncertainty about unobserved links describing plants feeding on animals. Similarly, if we observe two species which are normally obligate mutualists co-occurring but not interacting, we would be very doubtful that a zero reflecting this interaction was true (i.e., high uncertainty about this zero). It is straightforward to extend the intuition behind these special cases to the full set of species in a network using trait-based models, as long as the traits likely to affect interaction probabilities are known (e.g.,~\citet{Riede2011,Gravel2013,Bartomeus2016,Weinstein2017}). Models for interaction probabilities could also include information on abundances, phenological matching, co-occurrence etc. as available~\citep{Jordano2016,Weinstein2017a,Graham2018}. This approach allows us to tailor the level of uncertainty surrounding each interaction based on the species involved but involves many assumptions about which parameters affect interaction probabilities and the strength and shapes of these relationships. There is, therefore, a risk of over-fitting with complicated trait-based models. Also note that researchers wishing to test whether a particular trait influences interaction probabilities should either exclude that trait from their prior or model the network using priors which include and exclude the trait of interest (as in~\citet{Weinstein2017,Weinstein2017a}).


        Alternatively, it is possible to develop a prior based on the properties of a set of published networks (i.e., "reference Bayes"~\citet{Spiegelhalter2000}). While this type of prior does not allow for the prior distribution to vary between species pairs, it relies on fewer assumptions about the forces structuring interaction probabilities. Instead, we assume only that the focal network will have similar structural properties to those of published networks. For example, connectance ($C=N/S^2$, where $N$ is the number of interactions and $S$ is the number of species) is available for many networks. The distribution of these connectances can be used to predict the distribution of interaction probabilities in a new network. If we know the mean $\overline{C}$ and variance $\sigma_C^2$ of a distribution of connectances, then the hyperparameters are:

        \begin{equation}
        \alpha_{ij} = \overline{C}(\frac{\overline{C}(1-\overline{C})}{\sigma_C^2}-1) ,
        \end{equation}

        \begin{equation}
        \beta_{ij} = (1-\overline{C})(\frac{\overline{C}(1-\overline{C})}{\sigma_C^2}-1) .
        \end{equation}
  

        Note that, in this case, the prior distribution will be the same for all interaction probabilities. It is also possible to use other interaction probabilities. For example, species' degrees (numbers of interaction partners) in a similar network or networks could be normalised (divided by the number of potential interaction partners) and interpreted as an interaction probabilities (i.e., "empirical Bayes"~\citep{Spiegelhalter2000}). The distribution of these interaction probabilities can be used to inform a prior distribution as show in \emph{Appendix S3}. When using network summary statistics or degree distributions, it is important to consider whether the available information reflects what we truly believe about the network. Given the likelihood that most published networks are undersampled~\citep{Jordano2016}, it may be wise to include only the highest-quality networks available in a prior. Likewise, only the degree distribution for a similar system is likely to be a reasonable prior for a focal system. For example, if the system is a large network containing many plant species and insects from different families it is inappropriate to use a prior distribution drawn from a small network describing interactions between different genotypes of a single plant species and a set of closely-related insects (see \emph{Appendix S6} for a demonstration). The similarity of traits across genotypes within a species means that an insect which interacts with one genotype is likely to interact with many others. Trait differences between species mean that a species-species network will likely contain fewer interactions than a genotype-species network. Similarly, highly-aggregated networks containing nodes which represent whole classes or families are likely to have very different structures than networks where most nodes represent single species as aggregated nodes are likely to have more interaction partners than single species.


        If a similar dataset is not available, it is also possible to use data from the study at hand to develop a prior (i.e., "approximate Bayes"~\citet{Gelmanblog})~\citep{Casella1985,Spiegelhalter2000}. This approach is used in medical testing~\citep{Copas1972,Spiegelhalter2000} and disease modelling~\citep{Leyland2005,Brooks2015} using information from pilot studies and can easily be applied to ecological systems. Returning to the example of degree distributions, this approach allows us to use information from abundant and easily-sampled species to predict interaction probabilities for species which were observed only rarely. Despite its practicality, approximate Bayes has been criticized for being overly subjective~\citep{Berger2006} and because estimating hyperparameters from data serves as an approximation of some true hyperparameters which remain unknown~\citep{Gelmanblog}. We therefore encourage researchers to carefully consider whether external data or an appropriate trait model is available before opting for a prior based on some of their own data. Nevertheless, there are likely to be many situations where approximate Bayes provides the best way to describe the uncertainty in the dataset. We illustrate this approach in the following empirical example.


  \subsection*{An empirical example}

      To illustrate the process of constructing a Bayesian network to quantify uncertainty about interactions, we use the comprehensively-sampled system of willows (\emph{Salix}), herbivorous gallers, and their natural enemies described by~\citet{Kopelke2017}. This dataset consists of a single community type sampled across Europe over 29 years and at 374 unique locations. The meta-network consists of 1,173 different interactions between 52 \emph{Salix} nodes, 92 herbivore nodes, and 126 natural enemy nodes (see \emph{Appendix S4} for details). 
      The high spatiotemporal resolution of this dataset make it ideal for illustrating the difficulties in completely sampling a network; even with such an unusually high sampling effort, we anticipate that there will be many pairs of species which were rarely or never observed together. Using the Bayesian framework above, we can identify which potential interactions are more and less uncertain, allowing us to better predict the true structure of the metaweb. To show the gaps in sampling, we compared the frequencies of observed co-occurrences and interactions. We then calculated an empirical prior and computed the posterior distribution of the probability of an as-yet-unobserved interaction being feasible ($\lambda_{ij}$).
      We analysed both the \emph{Salix}-galler and galler-natural enemy components of the network but, for brevity, present only the latter here (see~\emph{Appendix S5} for \emph{Salix}-galler results).


    \subsubsection*{Computing the prior and posterior distributions}

        To treat each interaction as a Bayesian probability, we will combine the observed data with a distribution based on prior information. While a strict Bayesian framework requires a prior distribution that does not rely on any information from the study at hand, such data are not always available. In that case, researchers must choose between using a less-informative prior (e.g., one based on distributions of links across a large set of networks) or an empirical prior based on some "training data" from the study. Here, we opted for the latter and created priors using a single sub-network from the middle of the geographical distribution of the~\citet{Kopelke2017} dataset. To demonstrate how the use of data from a different system can affect the prior distribution and conclusions based on it, we repeated our analyses using priors derived from a much smaller \emph{Salix}-galler-natural enemy system based around genotypes of a single \emph{Salix} species~\citep[Data available from the Dryad Digital Repository: https://doi.org/10.5061/dryad.g7805]{Barbour2016}\nocite{Barbour2016Dryad}. This smaller system was much more densely-connected than that described in~\citet{Kopelke2017} and provided unreasonable distributions for interaction probabilities (\emph{Appendix S6}). 


        To obtain priors based on a single sub-network --Zillis in Graub\"{u}nchen, Switzerland-- we estimated frequencies of interactions based on the normalised degree of each species in each network component (see \emph{Appendix S7} for details and code). Using these prior parameters, we then estimated the posterior distributions of interaction probabilities $\lambda_{ij}$. For species without observed interactions ($n = 0$), the posterior distribution is identical to the prior distribution. For species where $n>0$, we can update the prior distribution with data. If we consider only pairs of species which were observed co-occurring but not interacting, $k_{ij}$ is always 0 and only $n_{ij}$ will vary between species pairs. This gives $\alpha_{ij}'$=$\alpha_{ij}$ and $\beta_{ij}'$=$\beta_{ij} + n_{ij}$. We calculated posterior distributions and 95\% credible intervals (see function ``credible\_interval";~\emph{Appendix S8}) for species with $n$ ranging between 0 and 374, the total number of sites in our dataset. 


        Rather than calculating credible intervals for a posterior distribution after collecting data, we may wish to know how many data points are necessary to obtain a given level of confidence that two co-occurring species do not interact. The number of samples needed will depend on both our desired level of confidence and the threshold below which we assume that two species are unlikely to ever interact. We calculated the number of samples required to reach 95\% confidence that $\lambda_{ij}$ was below thresholds of 0.1, 0.05, and 0.01 as examples (see function ``samples\_for\_threshold";~\emph{Appendix S8}).


    \subsubsection*{Scaling up to network metrics}

      Researchers are often interested in measures of network structure rather than the network itself. Computing most network metrics is straightforward when the different $\lambda_{ij}$ of the adjacency matrix are known and assumed not to vary~\citep{Poisot2016}. Incorporating variance in $\lambda_{ij}$ into these calculations, however, is not so easy. Computation of these metrics involves non-linear functions. By Jensen's inequality~\citep{Jensen1906}, this means that the average of a network metric (a function of stochastic interactions) is not the same as the network metric calculated based on the average interaction probability. Thus, any uncertainty in the values of $\lambda_{ij}$ could bias both the mean and variance of network metrics, giving misleading results (simulated in \emph{Appendix S1}). One way to avoid this situation is to calculate the properties of a suite of simulated networks~\citep{Vazquez2005,Guimera2009}.


      Using the prior distributions and procedures described above, we calculated posterior probability distributions for species pairs that were not observed interacting. Using these posterior distributions and assuming probabilities of 1 for pairs of species that were observed interacting (see \emph{Appendix S2} for a justification), we created a suite of 100 webs by randomly sampling from each posterior distribution. Each of these webs is a prediction of the structure of the metaweb. After obtaining these posterior networks, we calculated the connectance of each web, as well as the mean number of links per galler and per natural enemy, and the nestedness (NODF) of the network. This gives us an estimate for each property reflecting our uncertainty about $\lambda_{ij}$.


      Measures of network structure which are based on empirical networks that are missing interactions may differ substantially from the values that would be obtained if detection certainty and variation in interactions over space and time could be removed. To demonstrate this, we created a suite of filtered networks for each posterior network. Taking a posterior network as the "true" network, we randomly sampled 90\%, 80\%, 70\%, 60\%, 50\%, 40\%, 30\%, 20\%, and 10\% of the interactions to create a new "filtered" network. This gradient is akin to a gradient of uncertainty in the network (e.g., due to varying sampling effort or intraspecific trait variability). For each level of uncertainty, we created 100 randomly-sampled networks per posterior-probability network (giving 100 posterior networks and 1000 filtered networks). We calculated the same network properties as described above for all posterior and filtered networks.


\section*{Results}

  In the \emph{Salix}-based food webs sampled by~\citet{Kopelke2017}, most pairs of gallers and natural enemies (9,794/12,096) never co-occurred and, for species that did occur together, the total number of co-occurrences was generally low (mean=3.87; Fig.~\ref{histograms}A). The bulk (92.24\%) of these co-occurring species pairs were never observed interacting. Of those pairs that did interact, the incidence of interaction was also low (mean=4.04; Fig.~\ref{histograms}B) and was lower than the number of observed co-occurrences (Fig.~\ref{histograms}C).


  We obtained prior parameters of $\alpha_{ij}$=0.700, $\beta_{ij}$=8.49, giving a beta distribution strongly skewed towards 0. Where $n=0$, these parameters gave a posterior distribution with $\bar\lambda$=0.076, var($\lambda_{ij}$)=0.008. When $n=374$, we obtained a posterior distribution with $\bar\lambda_{ij}$=1.83 $\times$ 10$^{-3}$, var($\lambda_{ij}$)=4.76$\times10^6$. This distributions is very close to 0 with small variance about $\lambda_{ij}$; if species $i$ and $j$ co-occurred 374 times without interacting, they are extremely unlikely to do so at other sites or times. 


  For most pairs of species $i$ and $j$, $n_{ij}$ was much less than 374 and our posterior mean and variance therefore retain more of the influence of the prior. We can see this in the increasing means and variances as we decrease $n_{ij}$ (Fig.~\ref{Salix_pdfs_cdfs};~\emph{Appendix S9}). To be 95\% confident that the probability of interaction is below 0.1, 0.05, or 0.01 would require 15, 39, and 229 observed co-occurrences, respectively. Note that these are relatively large sample sizes compared to currently-available empirical networks (e.g.,~\citealp{Morris2014}).


  Scaling up to network structure, we found that the connectance and mean links per galler and natural enemy were much lower in the observed web (C=0.078, $L_{galler}$=9.99, and $L_{natural enemy}$=7.45, respectively) than in the posterior webs (0.186 $\leq$ C $\leq$ 0.198, 13.4 $\leq L_{galler} \leq$ 14.6, and 23.4 $\leq L_{natural enemy} \leq$ 25.0). When the detection probability was relatively low (i.e., 50\%), however, the properties of randomised networks became similar to those in the observed webs (Fig.~\ref{posterior_webs}A,B,D). Nestedness was higher in the observed network (NODF=6.85) than in the posterior webs (6.31 $\leq NODF \leq$ 6.82; Fig.~\ref{posterior_webs}C); in this case, the stronger the detection filter, the farther apart were the observed and posterior webs. In general, the observed network is most similar to simulated networks where only half of the plausible links are detected. 

  % Poisot2015 - also found that NODF was higher in the empirical data than in sims. 

\section*{Discussion}

  A benefit of Bayesian estimation is that it allows us to make our assumptions more explicit (e.g., assuming that funding source for a medical trial may affect the reported outcome~\citep{Spiegelhalter2000}).


  Real interaction networks vary over several dimensions~\citep{Kitching1987,Olesen2011a,Pires2011a,Baiser2012,Fodrie2015,Novak2015}, leading to pervasive under-estimation of interactions in published networks~\citep{Jordano2016}. Even in the most extensive data set that we could find, there was very little empirical data for each species pair. Most pairs of species were not observed co-occurring even once, less than 10\% of species pairs were observed interacting, and no pairs were observed to co-occur frequently enough to conclude that their probability of interacting was below 0.01. This suggests that limited sampling is a major source of uncertainty in empirical networks, in agreement with~\citet{Jordano2016,Weinstein2017a,Weinstein2017}. 


  Detectability estimated at 23.3\% in~\citet{Weinstein2017a}

  Although 374 sampling sites were not enough to obtain sufficient observations of all species pairs, we were able to reduce uncertainty about many potential interactions using a simple Bayesian framework. This framework allowed us to set appropriate confidence intervals about the probability of an unobserved interaction occurring, based on the distribution of interactions in one sample site. This gives us more reasonable estimates of interaction probabilities than assuming that no unobserved interactions actually occur. Such an approach is particularly useful when considering interactions involving species entering new ranges due to climate change or introductions. Understanding these species' interactions is important for a variety of conservation questions~\citep{Bartomeus2013,Gravel2013}, and a Bayesian approach using a trait-matching model or data from species' current ranges could help us to anticipate how species will integrate into new communities. 


  Our results also demonstrate how incomplete detection of interactions scales up to affect network structure. The structure of the well-sampled network used in our case study is most similar to the structure of simulated networks including only 50\% of the interactions suggested by the posterior distribution of interaction probabilities. This poor detection rate is similar to that reported in other studies~\citep{Bartomeus2013,Jordano2016,Weinstein2017a}. If our descriptions of empirical systems are generally missing so many feasible interactions, then it is vital to acknowledge this inherent uncertainty before comparing structures across systems or relating structure to stability.


  Based on the framework described above, we can now offer some recommendations for improved descriptions of ecological interactions. Observing all interactions in a network is likely impossible~\citep{Jordano2016}, but researchers can take steps to address the uncertainty this implies. First, sample sizes should be as large as possible in order to reduce detection uncertainty. Second, researchers should acknowledge the varying levels of confidence surrounding interactions between species pairs. Including the $n$ and $k$ values for each interaction will clearly indicate which unobserved interactions are most likely to be observed with further sampling and which estimates are more reliable, as well as indicating potential sampling biases. Where there are strong prior expectations about pairs of species that will not interact, these should be explicitly stated so that readers know which zeros in an interaction matrix are based on observed data and which are based primarily upon expert knowledge. Third, the uncertainty around interactions should be incorporated in calculations of network properties. Re-sampling networks based on a probabilistic understanding of networks is straightforward and gives distributions for network properties rather than point estimates. This not only acknowledges the fact that interactions vary over time and space but will also facilitate comparisons between networks by adding confidence intervals to estimates of network properties. This will allow us to say whether networks have different structures \emph{and} whether those differences are greater than we would expect given the inherent variability of interactions. To facilitate the practical application of these recommendations, we provide all code used in this paper in the supplementary material. 


\section*{Acknowledgements}

  The authors thank Daniel Cartensen for fruitful discussion of the ideas in this manuscript and K\'{e}vin Cazalles for providing feedback. The authors appreciate support from the Swedish Research Council (VR) for grant \#2016-06872 (to TR) and from Formas for grant \#942-2015-1262 (to AE).


\section*{Authors' contributions}

DG designed the analytical approach. AC and DG wrote the code. AC performed statistical analyses. All authors contributed to writing and revising the manuscript.


\section*{Data accessibility}

All data used in this study have been independently published and are accessible following the references provided in text.



\end{spacing}
\clearpage

% \captionsetup{singlelinecheck=false, font={stretch=2}}

\section*{Figures}

  \begin{figure}[h!]
    \caption{Three nested levels of uncertainty mean that an observed interaction matrix is unlikely to capture all of the interactions that can really occur at a site. Interaction uncertainty means that some interactions that truly occur (black squares in True matrix) are less likely than others (grey squares; lighter grey is less likely) based on species traits. For example, lions are less likely to predate upon elephants than zebras. If our trait model is incomplete (e.g., if we neglect group hunting in lions), we might assume that elephants are too large to ever be prey for lions and assign this interaction a probability of 0.\\
    \indent Process uncertainty describes the fact that interactions occur with different probabilities in any given sample. We show six example networks representing interactions observed over two days at three sites. Interactions which occur during fewer sampling days are less likely to be observed.\\
    \indent Detection uncertainty reflects the fact that not all interactions are equally detectable. For example, interactions involving cryptic species are less likely to be seen.\\
    \indent After applying these three layers of uncertainty, some interactions which truly occur are very unlikely to be observed. The observed matrix (lower right) is a subsample of the true matrix, with the probability of observing each interaction depending on the interaction, process, and detection uncertainty associated with it. Note that some low-probability interactions (e.g., lion predation on zebra) are included while others (e.g., lion predation on elephants) are not. For an illustration of the way in which these levels of uncertainty combine in simulated data, and the effects of uncertainty on network structure, see \emph{Appendix S1}. Attributions for PhyloPic silhouette images are given in \emph{Appendix S1-A}.}
    \label{conceptual_fig}
    \begin{center}
    \includegraphics*[height=.5\textheight]{figures/conceptual_fig.eps}
    % \includegraphics*[height=.5\textheight]{Figure_1.eps}
    \end{center}
    \end{figure}


  \begin{figure}[h!]
      \caption{Despite the replication in our empirical dataset, most galler-natural enemy pairs were never observed co-occurring and those that did co-occur rarely interacted. \textbf{A)} Here we show a histogram of the number of pairs of species observed co-occurring at least once. 9794 galler-enemy pairs were never observed co-occurring (not shown).
      \textbf{B)} Here we show a histogram of the number of observed interactions within pairs of co-occurring species. Species which co-occurred but never interacted are included. 
      \textbf{C)} Here we show, for each species pair, the number of observed interactions plotted against the number of observed co-occurrences. The red, dashed line indicates a 1:1 relationship between interactions and co-occurrences.}
      \label{histograms}
      \begin{center}
      \includegraphics*[height=.6\textheight]{figures/GP_histogram.eps}
      % \includegraphics*[height=.6\textheight]{Figure_2.eps}
      \end{center}
      \end{figure}


  \begin{figure}[h!]
    \caption{Imperfect detection of interactions increases the number of samples required to be confident that an interaction never occurs. Assume that two species cannot interact (i.e., $\lambda=0$) and that the number of observed interactions follows a binomial distribution depending upon the number of observed interactions $k$ and observed co-occurrances $n$. The maximum likelihood estimate (MLE) for $\lambda_{ij}$ is $\lambda_{MLE}=\frac{k}{n}$. Note that $\lambda_{ij}$ is not a point estimate but rather a random variable with an unknown distribution. This means that if $k = 0$ in a given sample, this does not necessarily imply that the two species will never interact. Rather, $k = 0$ implies that `no interaction' is the most likely outcome when the species do co-occur but there is nonetheless some chance that the two species \emph{could} interact.\\
    Importantly, we can estimate the variance of $\lambda_{ij}$ as well as the MLE; available methods include the \emph{Wilson score interval}, the \emph{Clopper-Pearson interval}, and the \emph{Agresti-Coull interval} (for details, see [\citealp{Brown2001}]). The distribution of the MLE for $\lambda_{ij}$ thus obtained is a key part of the Bayesian distribution for $\lambda_{ij}$.
    Here we show the upper bound (solid black line) of a 95\% Clopper-Pearson true credible interval for the interaction probability $\lambda_{ij}$ for a pair of species that has been observed co-occurring $n$ times but never interacting ($k = 0$). The upper limit of the credible interval only reaches 0.1 (dashed, red line) with 35 observations. Thus, adding more observations is useful in controlling uncertainty, but the number of observations added must be very high unless we have some other reason to believe that an interaction cannot occur (i.e., a strong prior).}
    \label{upper_limits}
    \begin{center}
    \includegraphics*[width=.8\textwidth]{figures/upper_limit_DG.eps}
    % \includegraphics*[width=.8\textwidth]{Figure_3.eps}
    \end{center}
    \end{figure}


  \begin{figure}[h!]
    \caption{Using prior distributions based on a single site (Zillis in Graub\"{u}nden, Switzerland) from~\citet{Kopelke2017}, we calculate posterior distributions for the probability of interaction ($\lambda_{ij}$) between two species that have not yet been observed interacting ($k_{ij} = 0$). 
    Posterior distributions (curves) and 95\% credible intervals (lines at top of panel) for $\lambda_{ij}$ narrow and approach zero as the number of observed co-occurrences ($n_{ij}$) increases. Diamonds indicate the maximum likelihood estimator for the mean probability of interaction.
    Dashed lines indicate threshold probabilities of 0.01, 0.05, and 0.1. The number of samples required to obtain a 95\% credible interval below each threshold increases rapidly. It takes just over 20 observed co-occurrences to be 95\% confident that $\lambda_{ij}<0.10$, approximately 50 co-occurrences to be 95\% confident that $\lambda_{ij}<0.05$, and over 100 co-occurrences to be 95\% confident that $\lambda_{ij}<0.01$.}
    \label{Salix_pdfs_cdfs}
    \begin{center}
    \includegraphics*[width=.8\textwidth]{figures/GP_pdfs_increasing_N_Zillis.eps}
    % \includegraphics*[width=.8\textwidth]{Figure_4.eps}
    \end{center}
    \end{figure}


  \begin{figure}[h!]
    \caption{Mean connectance, links per galler, nestedness (NODF), and links per natural enemy for networks assembled using posterior distributions based on a single site (Zillis in Graub\"{u}nden, Switzerland) from~\citet{Kopelke2017} occupied a narrow range. To obtain distributions of network properties, we created 100 ``posterior-sampling" networks and then, for each of these, created 100 ``filtered" networks by randomly sampling 50\%-99\% of the interactions included in the posterior-sampling network. This simulates imperfect detection of interactions in the field. Each point represents the mean (+/- SD) network property (e.g., connectance) obtained from a set of 100 filtered networks. The filtered networks cover a broader range of network properties than the posterior-sampling networks but the value of each property decreases as the strength of the detection filter increases. Values in the original web are indicated by dashed lines.}
    \label{posterior_webs}    
    \begin{center}
    \includegraphics[width=.8\textwidth]{figures/GP_posterior_properties_Zillis.eps}
    % \includegraphics[width=.8\textwidth]{Figure_5.eps}
    \end{center}
    \end{figure}


\clearpage

    \bibliographystyle{ecol_let} 
    \bibliography{manual_abbrev} % Abbreviate journal titles.


\end{document}



\documentclass[12pt]{article}
\usepackage{amsmath} 
\usepackage[dvips]{graphicx}
\usepackage{multirow} 
\usepackage{geometry} 
\usepackage{pdflscape}
\usepackage[labelfont=bf]{caption} 
\usepackage{setspace}
\usepackage[running]{lineno} 
% \usepackage[numbers,sort]{natbib}
\usepackage[round]{natbib} 
\usepackage{array}
\usepackage{hyperref,url}
\usepackage{float}
\usepackage{mdframed}
\makeatletter


% this creates a custom and simpler ruled box style
\newcommand\floatc@simplerule[2]{{\@fs@cfont #1 #2}\par}
\newcommand\fs@simplerule{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@simplerule
  \def\@fs@pre{\hrule height.8pt depth0pt \kern4pt}%
  \def\@fs@post{\kern4pt\hrule height.8pt depth0pt \kern4pt \relax}%
  \def\@fs@mid{\kern8pt}%
  \let\@fs@iftopcapt\iftrue}

% this code block defines the new and custom mdframed environment
\newmdenv[rightline=true,bottomline=true,topline=true,leftline=true,linewidth=2pt]{fullbox}


\newcommand{\methods}{\textit{Materials \& Methods}}
\newcommand{\SI}{\textit{Appendix}~}

\topmargin -1.5cm % 0.0cm 
\oddsidemargin 0.0cm % 0.2cm 
\textwidth 6.5in
\textheight 9.0in % 21cm
\footskip 1.0cm % 1.0cm

\usepackage{authblk}

\title{A quantitative framework for investigating the reliability of empirical network construction}


\author{Alyssa R. Cirtwill$^{1\dagger}$, \&  Anna Ekl\"{o}f$^{1}$, Tomas Roslin$^{2}$, Kate Wootton$^{2}$, Dominique Gravel$^{3}$}
\date{
% \begin{minipage}[h]{0.6\textwidth}
\small$^1$ Department of Physics,\\
Chemistry and Biology (IFM)\\ 
Link\"{o}ping University\\
Link\"{o}ping, Sweden\\
\medskip
\small$^2$ Department of Ecology\\ 
P.O. Box 7044\\ 
Swedish University of Agricultural Sciences \\ 
SE-750 07 Uppsala, Sweden\\
\medskip
\small$^3$ D\'{e}partement de biologie\\ 
Universit\'{e} de Sherbrooke\\ 
Sherbrooke, Canada
\medskip
\small$^\dagger$ Corresponding author:\\
alyssa.cirtwill@gmail.com\\
\medskip
\medskip
\normalsize Running head: Constructing quantitative interaction networks
% tel: +46 723 158464\\
% $^\ddagger$ anna.eklof@liu.se\\
% $^\star$ tomas.roslin@slu.se\\
% $^\diamond$ kate.wootton@slu.se\\
% $^\triangleright$ dominique.gravel@usherbrooke.ca
}


\renewcommand\Authands{ and }

\begin{document} 
\maketitle 
\raggedright
\setlength{\parindent}{15pt} 

\vspace{-.4in}

% {\small

% \section*{\small Details}

% \begin{minipage}[h]{0.6\textwidth}
% \begin{itemize}
% \item Running title: Quantitative network construction
% \item Article type: Ideas and Perspectives
% \item Number of references: 36
% \item Number of figs, tables, \& text boxes: 8 % Limit is 10
% \end{itemize}
% \end{minipage}\begin{minipage}[h]{0.4\textwidth}
% \begin{itemize}
% \item Text box 1 word count: 381
% \item Text box 2 word count: 210
% \item Abstract word count: 199
% \item Main text word count: 7078 % Including 31 page numbers. Limit is 6000-7000 including figs, tables, references. 
% \end{itemize}
% \end{minipage}
% }

\newpage

\begin{spacing}{2.0}

\section*{Abstract}

  \begin{enumerate}

    \item  Descriptions of ecological networks typically assume that the same interspecific interactions occur each time a community is observed. This contrasts with the known stochasticity of ecological communities: community composition, species abundances, and link structure all vary in space and time. Moreover, finite sampling generates variation in the set of interactions actually observed. For interactions that have not been observed, most data sets will not contain enough information for the ecologist to be confident that unobserved interactions truly did not occur.
    \item Here we develop the conceptual and analytical tools needed to capture uncertainty in the estimation of pairwise interactions. To define the problem, we identify the different contributions to the uncertainty of an interaction. We then outline a framework to quantify the uncertainty around each interaction by combining data on observed co-occurrences with prior knowledge. We illustrate this framework using the most extensively sampled network to date. 
    \item We found significant uncertainty in estimates for the probability of most pairwise interactions. This uncertainty can, however, be constrained with informative priors. This uncertainty scaled up to summary measures of network structure such as connectance or nestedness. By using an extremely well-sampled network as a test case, we find that even with informative priors we are likely to miss many interactions that may occur rarely or under different local conditions. 
    \item Overall, we demonstrate the utility of our approach and the importance of acknowledging the uncertainty inherent in network studies and the utility of treating interactions as probabilities in pinpointing areas where more study is needed. Most importantly, we stress that networks are best thought of as systems constructed from random variables, the stochastic nature of which must be acknowledged for an accurate representation. Doing so will fundamentally change networks analyses and yield greater realism.
  % Can be up to 350 words for MEE. This is 290 exactly.
\end{enumerate}


\section*{\small Keywords}

ecological networks; probabilistic interactions; Bayesian networks; sampling error; spatial variability; temporal variability; uncertainty

% \linenumbers
\clearpage

\section*{Introduction}

    Representing an ecological community as a network summarizes both species composition and interactions between species. A tabulation of the nodes (species) and their relative abundances forms the basis for traditional metrics of community composition such as alpha diversity. A network framework combines this tabulation of nodes with a list of interactions (links between nodes) so that networks provide additional, higher-order information on community structure. Despite this additional information, empirical descriptions of ecological networks are usually limited to static representations of the communities they describe. That is, whether the network is assembled based on aggregated data, a single intensive ``snapshot" sample, or expert knowledge, interactions are assumed to occur deterministically wherever and whenever the community is observed~\citep{Olesen2011a}. 


    The assumption of static communities contrasts with the known stochasticity of ecological communities~\citep{Gotelli2000}. Community composition and abundances vary over space~\citep{Baiser2012} and time~\citep{Olesen2011a}. Moreover, interactions vary over space~\citep{Kitching1987,Baiser2012}, time~\citep{Kitching1987,Olesen2011a}, and between individuals~\citep{Pires2011a,Fodrie2015,Novak2015}. Worse, variability in community composition and interactions may not be closely related. The removal of a species from a site will obviously also remove its interactions but, conversely, the co-occurrence of potentially interacting species does not guarantee that the species will interact at a given place and time. Interactions can be lost if the interaction partners remain present but are separated in time or are too rare to detect each other~\citep{Tylianakis2010}. Interactions can also fail to occur because of environmental contingencies~\citep{Poisot2015}, or because of individual preferences~\citep{Fodrie2015}. 


    Beyond ``true" variation in network structure, several researchers have noted the effects of sampling intensity (e.g.,~\citealp{Martinez1999,Bluthgen2006,Bluthgen2007}). An assessment of the accumulation of interactions with increasing sampling effort suggests that it is even more challenging to document interactions than species~\citep{Poisot2012}. As a result, it has been proposed that interactions should be described probabilistically and network metrics computed accordingly~\citep{Poisot2016}. Early work in this vein includes food-web models using likelihood-based approaches~\citep{Allesina2008} or Gaussian~\citep{Williams2010} or binomial~\citep{Rohr2016} probability functions for each possible interaction. These models may include information about species' traits~\citep{Rohr2016} or attempt to reproduce empirical network structures using a set of simple rules~\citep{Allesina2008,Williams2010}.


    Despite these efforts, we currently lack the quantitative methodology to deal with uncertainty generated by spatiotemporal variation in ecological interactions and by limited sampling. Even in extremely well-sampled networks, uneven sampling across species can lead to the inference that some species do not interact because they co-occur rarely or have not yet been observed together - even if they do interact when they do co-occur (see Fig.~\ref{histograms}). Nearly all network studies will thus neglect some interactions, necessitating an approach that acknowledges this uncertainty.


    In this study, we formalise the description of interactions between species as probabilities and develop analytical tools to capture the uncertainty in the estimation of these interactions. We focus on binary interactions as a first step, but the framework could be expanded to deal with interaction frequencies or strengths. To define the problem, we first identify different contributions to the uncertainty of an interaction and discuss the implications of each source of uncertainty for the properties of ecological networks. 
    % Next, we develop an analytical framework to quantify the uncertainty around interactions in an empirical web and, finally, illustrate this framework using an extensively sampled empirical network.
    % We illustrate this framework using the most extensively sampled network to date (Box 1).  Finally, we offer tangible recommendations for improved descriptors of ecological interactions. Through these efforts, we demonstrate both the utility of our approach and the importance of acknowledging the uncertainty inherent in network studies.


    \subsection*{Why do some interactions \emph{not} occur?}

      We start from the perspective of a community ecologist faced with the task of describing a previously unknown interaction network. This ecologist will be interested in generating a description of the species/nodes present and the links between them~\citep{Roslin2016}.  Importantly, the information sought is conveyed by both the presence and \emph{absence} of links. Presences and absences are not, however, equally certain. An observed link definitely occurred, but there are multiple reasons why a given link may not be observed \emph{whether or not the interaction truly occurred}. The detection of any interaction is a stochastic process. We define three nested levels of uncertainty contributing to this stochasticity: interaction uncertainty, process uncertainty, and detection uncertainty (Fig.~\ref{concept_fig}).


        \subsubsection*{Interaction uncertainty} 

          First, and most fundamentally, a pair of species may or may not have the appropriate characteristics (or traits) to interact. We define the probability of an interaction $L$ given characteristics $\mathbf{T}$ as $P(L | \mathbf{T})=\lambda$. Some species pairs, such as a salmon and a saguaro cactus, have incompatible traits and would never interact even if they should co-occur. In such cases, $\lambda = 0$ and there is no uncertainty. 


          Most species pairs, however, do not have traits that absolutely prevent an interaction. This means that it is generally unclear whether an unobserved interaction is impossible or simply a rare phenomenon with $\lambda>0$. This uncertainty is partly addressed by trait-matching models in which species with better-matched traits are more likely to interact~\citep{Bartomeus2016}. As these models improve and include better information about the influence of traits on interactions, the uncertainty will be reduced and $\lambda$ should either tend to 0 or to 1. Nevertheless, every model is imperfect and lacks information that could be used to define constraints on interactions~\citep{Dormann2017}. Some interaction uncertainty is therefore inevitable.


        \subsubsection*{Process uncertainty} 

         Even a feasible interaction, i.e. $L=1$, still may not occur at a given location or moment in time because of local constraints such as inclement weather or unsuitable habitat. We define the local realisation of an interaction, $X$, given that the interaction is feasible, as a stochastic process with associated probability $P(X|L=1)=\chi$. This phenomenon of interaction contingencies is usually not considered in network studies, but there is a rich literature in community ecology  on the topic. Phenological matching~\citep{MillerRushing2010,Gezon2016}, species preferences~\citep{Pires2011,Novak2015,Coux2016}, and fear effects~\citep{Luttbeg2005,Wirsing2008} are just some of the factors contributing to variation in the frequency of interactions between a given pair of species. Some of these factors can be addressed in mesocosm studies of networks (e.g., environmental conditions can be held stable) or could be included in models along the line of trait-matching models (e.g., phenological matching). In the field, however, process uncertainty is likely inevitable.


         One proposed solution to the problem of process uncertainty is to use repeated sampling of networks. For example,~\citet{Weinstein2017} used repeated sampling to estimate the daily probability of detecting an interaction and thereby model detection and process uncertainty. While conceptually attractive, this approach is unsuitable for interactions occurring over longer time scales (e.g., associations between hosts and parasitoids with a single generation per year), or very rare interactions which might not occur on any of the sampling days. Moreover, the faster accumulation of species than interactions means that repeated sampling will tend to introduce more unobserved interactions than observed interactions unless the number of samples is very high~\citep{Poisot2012}.


        \subsubsection*{Detection uncertainty} 

          Lastly, measurement errors are a pervasive source of uncertainty in the observation of ecological processes. Even if an interaction is feasible and occurs during sampling ($L = 1$ and $X = 1$), we may still fail to detect it. We may define the probability of detecting of an interaction, $D$, given the interaction is feasible and occurs, as a stochastic process with the associated probability $P(D|X=1,L=1)=\delta$. Detection failure could happen for many reasons (see~\citet{Wirta2014} for examples and partial solutions to them). Some sources of detection error can be minimised with appropriate sampling effort ($\delta$ will approach one with increasing number of samples), but other sources are often difficult to reduce (e.g. the occurrence of cryptic species might require molecular analysis for appropriate taxonomic identification as in~\citealt{Wirta2014,Frost2016}). The absence of an interaction on a given day could therefore be because it was not feasible [$P(L) = 0$], was feasible but did not occur because of local conditions during sampling ($P(X|L=1) = 0$], or was feasible and occurred during sampling but could not be observed [$P(D|X=1,L=1)=0$]. Because we cannot separate these possibilities, repeated sampling fails to satisfactorily address the uncertainty inherent in empirical networks.


    \subsection*{Estimating detection and process uncertainty}

        An observed interaction indicates that an interaction was feasible, occurred during sampling, and was detectable ($L = 1$, $X = 1$, and $D = 1$). An unobserved interaction could be due to any of the three sources of uncertainty ($D$, $X$, and/or $L = 0$). Only the case where $L = 0$ is an interaction that never occurs --the usual interpretation of a $0$ value in an interaction matrix. It is particularly important to rule out the situations where $D=0 \cup X = 1 \cup L=1$, i.e. where the interaction occurred at the location but was not observed, and $X = 0 \cup L =1$, i.e., where the interaction is feasible but did not occur at the local site (Fig.~\ref{conceptual_fig}). 
        % The occurrence of a true absence, our quantity of interest, corresponds to the joint event $L=0 \cup X=1 \cup D=1$ but %% AC: Doesn't a true absence just mean $L=0$ regardless of our ability to detect/local conditions? 
        This is not possible, however, using empirical data. An empirical ecologist will instead measure the marginal probability $P(L) = k/n$, where $k$ is the number of observed interactions and $n$ the number of observed co-occurrences. Given this information, the question becomes: how can we reduce the uncertainty around our estimated interaction probability?


        % The considerations above raise major challenges: when faced with empirical data, how may we infer whether unobserved interactions truly do not occur? How may we refine our sampling approaches to reduce uncertainties and gain insights into the impact of multiple processes on field observations? Importantly, some sources of uncertainty within these three broad groupings can be minimised with appropriate sampling (e.g., sampling in a variety of weather conditions, combining different methods of detection) while other sources are difficult or impossible to reduce since they are generated by the process in which we are interested (e.g., variation in individual preferences or traits). Given this multifaceted problem of uncertainty, what can we do to separate the different types of variation and reduce those that can be reduced?


        The obvious rule of thumb to reduce uncertainty is ``sample more". Increasing sample sizes will reduce uncertainty about the upper bound of the interaction probability and will also increase the probability of detecting unlikely or cryptic interactions (e.g., interactions where $L$=1 but process or detection uncertainty is high). Despite these benefits, we note that there are limits to the utility of increased sampling. Since the probability of observing the co-occurrence of two species will always be higher than the probability of observing their interaction (since the probability of interaction is conditional on both interaction partners being present), we will accumulate observations of co-occurrences faster than we will accumulate observations of interactions (Fig.~\ref{histograms}C). Thus, the more we sample, the more zeros will appear in our interaction matrix.


        An added complication is that increased sampling will not reduce uncertainty evenly across interactions. To record an interaction between A and B, we need to identify both partners correctly (a non-trivial problem in many food webs; e.g.~\citealp{Kaartinen2011,Roslin2016}). For both molecular and rearing techniques, certain types of interactions may go unnoticed due to technical challenges~\citep{Wirta2014}. This can bias the set of recorded interactions towards those that are easier to observe.


        Most importantly, if two species are never observed co-occurring during several days of sampling then repeated sampling reveals nothing about their probability of interacting if they should ever co-occur. Intensive sampling is, therefore, an important step towards reducing uncertainty in ecological networks but is unlikely to be enough in most cases. To supplement repeated sampling efforts and reduce uncertainty around unobserved interactions, we suggest researchers use prior knowledge of the same or similar systems. Here, we outline a Bayesian approach to combining repeated sampling and prior knowledge in probabilistic networks. We then demonstrate how to construct such a probabilistic network using an intensively and repeatedly sampled empirical dataset. Finally, we show how a probabilistic understanding of a network allows us to establish confidence intervals around measures of network structure.


\section*{Materials and methods}

  \subsection*{A Bayesian framework for interaction probabilities}

    We are most interested in how to quantify an interaction probability, and its associated uncertainty, for interactions that have not been observed. Assume that a pair of species has been observed co-occurring $n$ times and interacting in $k = 0$ cases. We want to know the probability $\lambda$ that the species would be observed interacting if not constrained by local conditions, imperfect detection, etc. We can consider the occurrence of an interaction as a Bernoulli trail where the number of successes ($k$) over $n$ trials will follow a binomial distribution (see~\emph{Appendix S1} for a full description of the mathematical framework). In this framework, the maximum likelihood estimate (MLE) for $\lambda$ is:

        \begin{equation}
          \lambda_{MLE} = \frac{k}{n}  .
          \label{theta_MLE}
        \end{equation}
  

    Note that $\lambda$ is not a point estimate but rather a random variable with an unknown distribution. This means that if $k = 0$ in a given sample, this does not necessarily imply that the two species will never interact. Rather, $k = 0$ implies that `no interaction' is the most likely outcome when the species do co-occur but there is nonetheless some chance that the two species \emph{could} interact. In the situation where $k>0$, in contrast, we are sure that the interaction is feasible ($L = 1$). If $n>k>0$, the interaction is feasible but there may be local constraints ($X = 0$) or detection errors ($D<1$) causing the interaction not to be observed in some samples. 


    To properly interpret $\lambda$, we need to estimate the variance as well as the MLE. The variance of a Bernoulli experiment is $n\lambda$(1-$\lambda$) and, importantly, describes the variability of the number of successes $k$ for $n$ trials rather than the variance associated with the estimation of $\lambda$. It is, however, straightforward to compute the confidence interval for the MLE of $\lambda$ using any of several methods, including the \emph{Wilson score interval}, the \emph{Clopper-Pearson interval}, and the \emph{Agresti-Coull interval} (for details, see [\citealp{Brown2001}]). 


    All of the above methods for estimating the variance of $\lambda$ include the number of samples $n$. This means that where the number of samples $n$ is very low (e.g., for rare species), there will be considerable uncertainty around our estimate of $\lambda$. In Fig.~\ref{upper_limits}, we derive the Clopper-Pearson interval to explore how the estimate of $\lambda$ (for true $\lambda = 0$) varies with sample size. At a small sample size, the 95\% confidence interval is nearly (0, 1). To establish that species are not interacting with any acceptable certainty requires tens of observations of the two species co-occurring but not interacting. Most data sets will lack such extensive sampling across all species pairs, but we can still use a Bayesian approach to supplement the available data with other sources of information.


    The Bayesian approach combines the MLE of $\lambda$ described above with a prior distribution and a normaliser to give a posterior distribution. Since $\lambda$ is a probability and bounded between 0 and 1, the most appropriate prior distribution is the beta: 

    \begin{equation}
          \lambda \sim Beta(\alpha,\beta) , \label{prior}
        \end{equation}

        \noindent which has two shape parameters, $\alpha$ and $\beta$. 

    Combining the beta-distributed prior with the binomial-distributed MLE of $\lambda$ is straightforward (see~\emph{Appendix S1}). By doing so, we obtain a posterior distribution that follows a beta distribution with new parameters $\alpha'= \alpha+k$ and $\beta'=\beta+n-k$. The weight of the prior on the posterior distribution can be understood from these parameter definitions: the difference between the posterior and the prior will increase with $k$ and $n-k$. In other words, the distribution of $\lambda$ for better-sampled pairs of species will rely less on the information used to build the prior distribution and depend more on the observed data. When plotted, we find the shape of the distribution gets narrower with $k$ and $n$ (Fig.~\ref{Salix_pdfs_cdfs}). 


    \subsubsection*{Moments and other properties}

      It is common to perform analyses that require calculating higher-order network properties in interaction networks. The fact that the posterior distribution of $\lambda$ follows a beta distribution makes it straightforward to compute moments and other properties needed for these analyses. 


      The \textbf{average} of $\lambda$ is: 
          \begin{equation}
            \bar{\lambda} = \frac{\alpha+k}{\alpha+\beta+n} ,
            \label{mean}
          \end{equation}

        and its \textbf{variance} is:  
          \begin{equation}
            Var(\lambda|k) = \frac{(\alpha + k)(\beta + n - k)}{(\alpha + \beta + n)^{2}(\alpha + \beta + n +1)}
            \label{variance}
          \end{equation}

        The \textbf{mode} of the distribution is:
          \begin{equation}
            \hat{\lambda} = \frac{\alpha + k - 1}{\alpha + \beta + n - 2} .
            \label{mode}
          \end{equation}


    \subsubsection*{Choosing a prior distribution}    

      Parameters $\alpha$ and $\beta$ determine the shape of the prior distribution of $\lambda$, which follows a beta distribution. These are called hyper parameters. The prior may be chosen to be uninformative, if no external information is available, or may draw on information such as distributions of connectance across webs, degree distributions, or trait-matching functions (see \emph{Appendix S2} for details and a simple quantitative example). 


      It is important to ensure that an informative prior accurately reflects the focal  system. For example, if the system is a large network containing many plant species and insects from different families, it is inappropriate to use a prior distribution drawn from a small network describing interactions between different genotypes of a single plant species and a set of closely-related insects. The similarity of traits across genotypes within a species means that an insect which interacts with one genotype is likely to interact with many others. Trait differences between species mean that a species-species network will likely contain fewer interactions than a genotype-species network. Similarly, highly-aggregated networks containing nodes which represent whole classes or families are likely to have very different structures than networks where most nodes represent single species as aggregated nodes are likely to have more interaction partners than single species.


      The choice of prior will depend on the information available. In general, it is best to use a prior taken from a similar system as emphasized above. If such data are not available, a prior based on many networks with a similar level of resolution could be useful, as could a prior based on a preliminary round of sampling (analogous to ``training data"). If there is no suitable informative prior, it is best to use an explicitly uninformative prior. While an uninformative prior will not greatly reduce variance about $\lambda$, it does clearly state our lack of prior knowledge and will not bias our conclusions.


  \subsection*{An empirical example}

      To illustrate the process of constructing a Bayesian network, we use the well-sampled system of willows (\emph{Salix}), herbivorous gallers, and their natural enemies described by~\citet{Kopelke2017}. This dataset consists of a single community type sampled across Europe over 29 years and at 374 unique locations. The data were collected over 29 years at 374 unique locations across Europe. The meta-network consists of 1,173 different interactions between 52 \emph{Salix} nodes, 92 herbivore nodes, and 126 natural enemy nodes (see \emph{Appendix S3} for details). 


      The high spatiotemporal resolution and unusally high sampling effort of this dataset make it well suited for illustrating the difficulties in completely sampling a network. To show the gaps in sampling, we compared the frequencies of observed co-occurrences and interactions. We analysed both the \emph{Salix}-galler and galler-natural enemy components of the network but, for brevity, present only the latter here (see~\emph{Appendix S4} for \emph{Salix}-galler results). 


    \subsubsection*{Computing the prior and posterior distributions}

        To treat each interaction as a Bayesian probability, we will combine the observed data with a distribution based on prior information. A strict Bayesian framework requires a prior distribution that does not rely on any information from the study at hand. If such data are not available, one might instead use the first sub-network collected as ``training data" to guide future sampling. To simulate this situation, we created priors using a single sub-network from the middle of the geographical distribution of the~\citet{Kopelke2017} dataset. To demonstrate how the use of data from a different system can affect the prior distribution and conclusions based on it, we repeated our analyses using priors derived from a much smaller \emph{Salix}-galler-natural enemy system based around genotypes of a single \emph{Salix} species~\citep[Data available from the Dryad Digital Repository: https://doi.org/10.5061/dryad.g7805]{Barbour2016}\nocite{Barbour2016Dryad}. This smaller system was much more densely-connected than that described in~\citet{Kopelke2017} and provided unreasonable distributions for interaction probabilities (\emph{Appendix S5}). 


        To obtain priors based on a single sub-network --Zillis in Graub\"{u}nchen, Switzerland-- we estimated frequencies of interactions based on the normalised degree of each species in each network component (see \emph{Appendix S6} for details and code). Using these prior parameters, we then estimated the posterior distributions of interaction probabilities $\lambda$. For species without observed interactions ($n = 0$), the posterior distribution is identical to the prior distribution. For species where $n>0$, we can update the prior distribution with data. If we consider only pairs of species which were observed co-occurring but not interacting, $k_{ij}$ is always 0 and only $n_{ij}$ will vary between species pairs. This gives $\alpha'$=$\alpha$ and $\beta'$=$\beta + n_{ij}$. We calculated posterior distributions and 95\% credible intervals (see function ``credible\_interval";~\emph{Appendix S7}) for species with $n$ ranging between 0 and 374, the total number of sites in our dataset. 


        Rather than calculating credible intervals for a posterior distribution after collecting data, we may wish to know how many data points are necessary to obtain a given level of confidence that two co-occurring species do not interact. The number of samples needed will depend on both our desired level of confidence and the threshold below which we assume that two species are unlikely to ever interact. We calculated the number of samples required to reach 95\% confidence that $\lambda$ was below thresholds of 0.1, 0.05, and 0.01 as examples (see function ``samples\_for\_threshold";~\emph{Appendix S7}).


    \subsubsection*{Scaling up to network metrics}

      Researchers usually use interaction matrices as a tool to calculate measures of network structure. Computing most network metrics is straightforward when the different $\lambda$ of the adjacency matrix are known and assumed not to vary~\citep{Poisot2016}. Incorporating variance in $\lambda$ into these calculations, however, is not so easy. Computation of these metrics involves non-linear functions. By Jensen's inequality~\citep{Jensen1906}, this means that the average of a network metric (a function of stochastic interactions) is not the same as the network metric calculated based on the average interaction probability. Any uncertainty in the values of $\lambda$ could bias both the mean and variance of network metrics, giving misleading results (simulated in \emph{Appendix S8}). One way to avoid this situation is to calculate the properties of a suite of simulated networks.


      Using the prior distributions and procedures described above, we calculated posterior probability distributions for species pairs that were not observed interacting. Using these posterior distributions and assuming probabilities of 1 for pairs of species that were observed interacting, we created a suite of 100 webs by randomly sampling from each posterior distribution. After obtaining these posterior networks, we calculated the connectance of each web, as well as the mean number of links per galler and per natural enemy, and the nestedness (NODF) of the network. To demonstrate how these network metrics will be affected by detection uncertainty, we then created a suite of filtered networks for each posterior network. Networks were filtered by randomly sampling 99\%, 95\%, 90\%, 80\%, 70\%, 60\%, and 50\% of the interactions included in each posterior network. This gradient is akin to a gradient of sampling effort. For each level of detection accuracy, we created 100 randomly-sampled networks per posterior-probability network (giving 100 posterior networks and 1000 detection-filtered networks). We calculated the same network properties as described above for all posterior and detection-filtered networks.


\section*{Results}

  Most pairs of species (9,794/12,096) never co-occurred and, for species that did occur together, the total number of co-occurrences was generally low (mean=3.87; Fig.~\ref{histograms}A). The bulk (92.24\%) of these co-occurring species pairs were never observed interacting. Of those pairs that did interact, the incidence of interaction was also low (mean=4.04; Fig.~\ref{histograms}B) and was lower than the number of observed co-occurrences (Fig.~\ref{histograms}C).


  We obtained prior parameters of $\alpha$=0.700, $\beta$=8.49. Where $n = 0$, these parameters gave a posterior distribution with $\bar\lambda$=0.076, var($\lambda$)=0.008. When $n = 374$, we obtained a posterior distribution with $\bar\lambda_{ij}$=1.83 $\times$ 10$^{-3}$, var($\lambda_{ij}$)=4.76. This distributions is very close to 0 with small variance about $\lambda$; if species $i$ and $j$ co-occurred 374 times without interacting, they   are extremely unlikely to do so at other sites or times. 


  For most pairs of species $i$ and $j$, $n_{ij}$ was much less than 374 and our posterior mean and variance therefore retain more of the influence of the prior. We can see this in the increasing means and variances as we decrease $n_{ij}$ (Fig.~\ref{Salix_pdfs_cdfs};~\emph{Appendix S9}). To be 95\% confident that the probability of interaction is below 0.1, 0.05, or 0.01 would require 15, 39, and 229 observed co-occurrences, respectively. Note that these are relatively large sample sizes compared to currently-available empirical networks~\citep{Morris2014}.


  Scaling up to network structure, we found that the connectance and mean links per galler and natural enemy were much lower in the observed web (C=0.078, $L_{galler}$=9.99, and $L_{natural enemy}$=7.45, respectively) than in the posterior webs (0.186 $\leq$ C $\leq$ 0.198, 13.4 $\leq L_{galler} \leq$ 14.6, and 23.4 $\leq L_{natural enemy} \leq$ 25.0). When the detection probability was relatively low (i.e., 50\%), however, the properties of randomised networks became similar to those in the observed webs (Fig.~\ref{posterior_webs}A,B,D). Nestedness was higher in the observed network (NODF=6.85) than in the posterior webs (6.31 $\leq NODF \leq$ 6.82; Fig.~\ref{posterior_webs}C); in this case, the stronger the detection filter, the farther apart were the observed and posterior webs. In general, the observed network is most similar to simulated networks where only half of the plausible links are detected. 


\section*{Discussion}

  Real interaction networks vary over several dimensions~\citep{Kitching1987,Olesen2011a,Pires2011a,Baiser2012,Fodrie2015,Novak2015}, leading to pervasive under-estimation of interactions in published networks. Even in the most extensive data set that we could find, there was very little empirical data for each species pair. Most pairs of species were not observed co-occurring even once, less than 10\% of species pairs were observed interacting, and no pairs were observed to co-occur frequently enough to conclude that their probability of interacting was below 0.01. This suggests that limited sampling is a major source of uncertainty in empirical networks. This dataset also illustrates the problem of process uncertainty - pairs of species which were observed interacting did not interact in all of the samples in which they co-occurred. Increasing sampling is therefore likely to add more zeros than ones to an interaction network.


  Using prior knowledge to build a Bayesian distribution for each interaction probability allows us to set appropriate bounds on interaction probabilities for species which were rarely or never observed co-occurring. In such cases, an informative prior distribution gives us more reasonable estimates of interaction probabilities than assuming that $p$=0 for unobserved interactions. This approach is particularly useful when considering interactions involving species entering new ranges due to climate change or introductions. Understanding these species' interactions is important for a variety of conservation questions, and a Bayesian approach using a trait-matching model or data from species' current ranges could help us to anticipate how species will integrate into new communities. 


  Our results also demonstrate how incomplete detection of interactions scales up to affect network structure. The structure of the well-sampled network used in our case study is most similar to the structure of simulated networks including only 50\% of the interactions suggested by the posterior distribution of interaction probabilities. If our descriptions of empirical systems are missing up to half of the feasible interactions, then it is vital to acknowledge this inherent uncertainty before comparing structures across systems or relating structure to stability.


  Based on the framework described above, we can now offer some recommendations for improved descriptions of ecological interactions. First, sample sizes should be as large as possible in order to reduce detection uncertainty. Second, where sampling on the order of 30-50 observations per species pair is not possible (i.e., in most cases), researchers should acknowledge the varying levels of confidence surrounding interactions between different species pairs. Including the $n$ and $k$ values for each interaction will clearly indicate which unobserved interactions are most likely to be observed with further sampling and which estimates are more reliable. Third, the uncertainty around interactions should be incorporated in calculations of network properties. Re-sampling networks based on a probabilistic understanding of networks is straightforward and gives distributions for network properties rather than point estimates. This not only acknowledges the fact that interactions vary over time and space but will also facilitate comparisons between networks by adding confidence intervals to ezostimates of network properties. This will allow us to say whether networks have different structures \emph{and} whether those differences are greater than we would expect given the inherent variability of interactions. To facilitate the practical application of these recommendations, we provide all code used in this paper in the supplementary material. 


\section*{Acknowledgements}

  The authors thank Daniel Cartensen for fruitful discussion of the ideas in this manuscript and K\'{e}vin Cazalles for providing feedback. The authors appreciate support from the Swedish Research Council (VR) for grant \#2016-06872 (to TR) from Formas for grant \#942-2015-1262 (to AE).


\section*{Authors' contributions}

DG designed the analytical approach. AC and DG wrote the code. AC performed statistical analyses. All authors contributed to writing and revising the manuscript.


\section*{Data accessibility}

All data used in this study have been independently published and are accessible following the references provided in text.



\end{spacing}
\clearpage

\captionsetup{singlelinecheck=false, font={stretch=1}}

\section*{Figures}


% \begin{fullbox}{}
%   \begin{spacing}{1.0}
%     \textbf{Box 1: }\emph{Salix}-galler-natural enemy dataset.\\
%     \indent As a case study, we use an extensively sampled \emph{Salix}-galler-natural enemy meta-network. This dataset consists of a single community type sampled across Europe: willow (\emph{Salix}) species, willow-galling sawflies, and their natural enemies. The data were collected over 29 years at 374 unique locations across Europe. The meta-network consists of 1,173 different interactions between 52 \emph{Salix} nodes, 92 herbivore nodes, and 126 natural enemy nodes. The high spatiotemporal resolution of this network and the unusually high sampling effort implemented at the site level makes this dataset particularly well suited for illustrating the difficulties in completely sampling a network and testing Bayesian approaches to overcome these difficulties.\\
%     \indent We may begin by comparing the frequencies of co-occurrences and interactions to reveal the challenge of having sufficient sampling to be confident that an interaction does not occur. Most pairs of species (9,794/12,096 galler-natural enemy pairs) are never found co-occurring and, for species that did occur together, the total number of co-occurrences was generally low (mean=3.87, variance=28.8; Fig.~\ref{histograms}A). The bulk of these co-occurring species pairs were never observed to interact: only 7.76\% of galler-natural enemy pairs were observed interacting. Of those pairs that did interact, the incidence of interaction was also low (mean=4.04, variance=29.3; Fig.~\ref{histograms}B). Thus, even in the most extensive data set that we could find, there was very little empirical data for each species pair. This suggests that limited sampling is a major source of uncertainty in all empirical networks. This dataset also illustrates the potential for increased sampling to not necessarily reveal more interactions as a pair of species that is able to interact may not be observed interacting in all samples where the pair co-occurs (Fig.~\ref{histograms}C).
%   \end{spacing}
% \end{fullbox}

% \clearpage

  \begin{figure}[h!]
    \caption{Three nested levels of uncertainty mean that an observed interaction matrix is unlikely to capture all of the interactions that can really occur at a site. Interaction uncertainty means that some inteactions that truly occur (black squares in True matrix) are less likely than others (grey squares) based on species traits. For example, lions are less likely to predate upon elephants than zebras. If our trait model is incomplete (e.g., if we neglect group hunting in lions), we might assume that elephants are too large to ever be prey for lions and assign this interaction a probability of 0.\\
    Process uncertainty describes the fact that interactions occur with different probabilities during any given sample. We show six example networks representing interactions observed over two days at three sites. Interactions which occur during fewer sampling days are less likely to be observed.\\
    Detection uncertainty reflects the fact that not all interactions are equally detectable. For example, interactions involving cryptic species are less likely to be seen.\\
    After applying these three layers of uncertainty, some interactions which truly occur are very unlikely to be observed. An observed matrix sampled based on this matrix (lower left) is thus likely to contain far fewer interactions than the original, true matrix.}
    \label{conceptual_fig}
    \begin{center}
    % \includegraphics*[height=.5\textheight]{figures/conceptual_fig.eps}
    \end{center}
    \end{figure}


  \begin{figure}[h!]
      \caption{Despite the replication in our empirical dataset, most galler-natural enemy pairs were never observed co-occurring and those that did co-occur rarely interacted. \textbf{A)} Here we show a histogram of the number of pairs of species observed co-occurring at least once. 9794 galler-enemy pairs were never observed co-occurring (not shown).
      \textbf{B)} Here we show a histogram of the number of observed interactions within pairs of co-occurring species. Species which co-occurred but never interacted are included. 
      \textbf{C)} Here we show, for each species pair, the number of observed interactions plotted against the number of observed co-occurrences. The red, dashed line indicates a 1:1 relationship between interactions and co-occurrences.}
      \label{histograms}
      \begin{center}
      % \includegraphics*[height=.6\textheight]{figures/GP_histogram.eps}
      \end{center}
      \end{figure}


  \begin{figure}[h!]
    \caption{Imperfect detection of interactions increases the number of samples required to be confident that an interaction never occurs. Assume that two species cannot interact (i.e., $\lambda=0$) and that the number of observed interactions follows a binomial distribution. Here we show the upper bound (solid black line) of a 95\% Cloper-Pearson true credible interval for the interaction probability $\lambda$ for a pair of species that has been observed co-occurring $n$ times but never interacting ($k = 0$). The upper limit of the credible interval only reaches 0.1 (dashed, red line) with 35 observations. Thus, adding more observations is useful in controlling uncertainty, but the number of observations added must be very high. }
    \label{upper_limits}
    \begin{center}
    % \includegraphics*[width=.8\textwidth]{figures/upper_limit_DG.eps}
    \end{center}
    \end{figure}


  \begin{figure}[h!]
    \caption{Using prior distributions based on a single site (Zillis in Graub\"{u}nden, Switzerland) from~\citet{Kopelke2017}, we calculate posterior distributions for the probability of interaction ($\lambda$) between two species that have not yet been observed interacting ($k = 0$). 
    Posterior distributions (curves) and 95\% credible intervals (lines at top of panell) for $\lambda$ narrow and approach zero as the number of observed co-occurrences ($n$) increases. Diamonds indicate the maximum likelihood estimator for the mean probability of interaction.
    Dashed lines indicate threshold probabilities of 0.01, 0.05, and 0.1. The number of samples required to obtain a 95\% credible interval below each threshold increases rapidly. It takes just over 20 observed co-occurrences to be 95\% confident that $\lambda<0.10$, approximately 50 co-occurrences to be 95\% confident that $\lambda<0.05$, and over 100 co-occurrences to be 95\% confident that $\lambda<0.01$.}
    \label{Salix_pdfs_cdfs}
    \begin{center}
    % \includegraphics*[width=.8\textwidth]{figures/GP_pdfs_increasing_N_Zillis.eps}
    \end{center}
    \end{figure}

  % \begin{figure}[h!]
  %   \caption{The number of samples required to achieve a given level of confidence that an interaction probability $\lambda_{ij}$ is below a given threshold varies with both parameters. With a low threshold, our confidence that $\lambda_{ij}$ is below the threshold increases rapidly with repeated observation of co-occurrence without interaction. Here we show the cumulative density functions for threshold probabilities of 0.5 (solid line), 0.25 (dashed line), 0.1 (dash-dot line), and 0.05 (dotted line) as well as the points at which the cdf reaches 0.90 (orange square), 0.95 (red circle), and 0.975 (blue diamond) for each threshold value. The large ticks along the x-axis indicate the number of samples associated with each of these points. \textbf{A)} In the \emph{Salix}-galler network component, the 95\% credible interval for $\lambda_{ij}$ when $n$=0 was (0.013, 0.049). We can therefore be at least 95\% confident that $\lambda_{ij}$ is below thresholds of 0.1 or 0.05 without any observed co-occurrence of species $i$ and $j$. To be confident that $\lambda_{ij}$ is less than 0.01, however, would require more observed co-occurrences than there are sites in our dataset. \textbf{B)} In the galler-parasitoid network component, the 95\% credible interval for $\lambda_{ij}$ was substantially broader and many observed co-occurrences ($\approx$ 15-35) are required to be 95\% confident that $\lambda_{ij}$ is below thresholds of 0.1 or 0.05.}
  %   \label{Salix_cdfs}
  %   \begin{center}
  %   \includegraphics[width=.5\textwidth]{figures/Salix_Galler_samples_and_cdfs_Zillis.eps}
  %   \end{center}

  %   \end{figure}


  \begin{figure}[h!]
    \caption{Mean connectance, links per galler, nestedness (NODF), and links per natural enemy for networks assembled using posterior distributions based on a single site (Zillis in Graub\"{u}nden, Switzerland) from~\citet{Kopelke2017} occupied a narrow range. To obtain distributions of network properties, we created 100 ``posterior-sampling" networks and then, for each of these, created 100 ``detection-filter" networks by randomly sampling 50\%-99\% of the interactions included in the posterior-sampling network. This simulates imperfect detection of interactions in the field. Each point represents the mean (+/- SD) network property (e.g., connectance) obtained from a set of 100 detection-filter networks. The detection-filter networks cover a broader range of network properties than the posterior-sampling networks but the value of each property decreases as the strength of the detection filter increases. Values in the original web are indicated by dashed lines.}
    \label{posterior_webs}    
    \begin{center}
    % \includegraphics[width=.8\textwidth]{figures/GP_posterior_properties_Zillis.eps}
    \end{center}
    \end{figure}


\clearpage

    \bibliographystyle{ecol_let} 
    \bibliography{manual_abbrev} % Abbreviate journal titles.


\end{document}



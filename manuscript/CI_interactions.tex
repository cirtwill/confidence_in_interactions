\documentclass[12pt]{article}
\usepackage{amsmath} 
\usepackage[dvips]{graphicx}
\usepackage{multirow} 
\usepackage{geometry} 
\usepackage{pdflscape}
\usepackage[labelfont=bf]{caption} 
\usepackage{setspace}
\usepackage[running]{lineno} 
% \usepackage[numbers,sort]{natbib}
\usepackage[round]{natbib} 
\usepackage{array}
\usepackage{hyperref,url}
\usepackage{float}
\usepackage{mdframed}
\makeatletter


% this creates a custom and simpler ruled box style
\newcommand\floatc@simplerule[2]{{\@fs@cfont #1 #2}\par}
\newcommand\fs@simplerule{\def\@fs@cfont{\bfseries}\let\@fs@capt\floatc@simplerule
  \def\@fs@pre{\hrule height.8pt depth0pt \kern4pt}%
  \def\@fs@post{\kern4pt\hrule height.8pt depth0pt \kern4pt \relax}%
  \def\@fs@mid{\kern8pt}%
  \let\@fs@iftopcapt\iftrue}

% this code block defines the new and custom mdframed environment
\newmdenv[rightline=true,bottomline=true,topline=true,leftline=true,linewidth=2pt]{fullbox}


\newcommand{\methods}{\textit{Materials \& Methods}}
\newcommand{\SI}{\textit{Appendix}~}

\topmargin -1.5cm % 0.0cm 
\oddsidemargin 0.0cm % 0.2cm 
\textwidth 6.5in
\textheight 9.0in % 21cm
\footskip 1.0cm % 1.0cm

\usepackage{authblk}

\title{A quantitative framework for investigating the reliability of empirical network construction}


\author{Alyssa R. Cirtwill$^{1\dagger}$, \&  Anna Ekl\"{o}f$^{1}$, Tomas Roslin$^{2}$, Kate Wootton$^{2}$, Dominique Gravel$^{3}$}
\date{
% \begin{minipage}[h]{0.6\textwidth}
\small$^1$ Department of Physics,\\
Chemistry and Biology (IFM)\\ 
Link\"{o}ping University\\
Link\"{o}ping, Sweden\\
\medskip
\small$^2$ Department of Ecology\\ 
P.O. Box 7044\\ 
Swedish University of Agricultural Sciences \\ 
SE-750 07 Uppsala, Sweden\\
\medskip
\small$^3$ D\'{e}partement de biologie\\ 
Universit\'{e} de Sherbrooke\\ 
Sherbrooke, Canada
\medskip
\small$^\dagger$ Corresponding author:\\
alyssa.cirtwill@gmail.com\\
\medskip
\medskip
\normalsize Running head: Constructing quantitative interaction networks
% tel: +46 723 158464\\
% $^\ddagger$ anna.eklof@liu.se\\
% $^\star$ tomas.roslin@slu.se\\
% $^\diamond$ kate.wootton@slu.se\\
% $^\triangleright$ dominique.gravel@usherbrooke.ca
}


\renewcommand\Authands{ and }

\begin{document} 
\maketitle 
\raggedright
\setlength{\parindent}{15pt} 

\vspace{-.4in}

% {\small

% \section*{\small Details}

% \begin{minipage}[h]{0.6\textwidth}
% \begin{itemize}
% \item Running title: Quantitative network construction
% \item Number of references: 37
% \item Number of figs, tables, \& text boxes: 6 % Limit is 10
% \end{itemize}
% \end{minipage}\begin{minipage}[h]{0.4\textwidth}
% \begin{itemize}
% \item Abstract word count: 290
% \item Main text word count: 6872 % Limit is 6000-7000 including figs, tables, references. 
% \end{itemize}
% \end{minipage}
% }

\newpage

\begin{spacing}{2.0}

\section*{Abstract}
  [[was fine? Maybe lightly edit to make clear we're not doing a review]]
  \begin{enumerate}

    \item  Descriptions of ecological networks typically assume that the same interspecific interactions occur each time a community is observed. This contrasts with the known stochasticity of ecological communities: community composition, species abundances, and link structure all vary in space and time. Moreover, finite sampling generates variation in the set of interactions actually observed. For interactions that have not been observed, most data sets will not contain enough information for the ecologist to be confident that unobserved interactions truly did not occur.
    \item Here we develop the conceptual and analytical tools needed to capture uncertainty in the estimation of pairwise interactions. To define the problem, we identify the different contributions to the uncertainty of an interaction. We then outline a framework to quantify the uncertainty around each interaction by combining data on observed co-occurrences with prior knowledge. We illustrate this framework using perhaps the most extensively sampled network to date. 
    \item We found significant uncertainty in estimates for the probability of most pairwise interactions. This uncertainty can, however, be constrained with informative priors. This uncertainty scaled up to summary measures of network structure such as connectance or nestedness. By using a comprehensively sampled network as a test case, we find that even with informative priors we are likely to miss many interactions that may occur rarely or under different local conditions. 
    \item Overall, we demonstrate the utility of our approach, given the importance of acknowledging the uncertainty inherent in network studies, and the utility of treating interactions as probabilities in pinpointing areas where more study is needed. Most importantly, we stress that networks are best thought of as systems constructed from random variables, the stochastic nature of which must be acknowledged for an accurate representation. Doing so will fundamentally change networks analyses and yield greater realism.
  % Can be up to 350 words for MEE.
\end{enumerate}


\section*{\small Keywords}

ecological networks; probabilistic interactions; Bayesian networks; sampling error; spatial variability; temporal variability; uncertainty

\linenumbers
\clearpage

\section*{Introduction}

    Representing an ecological community as a network allows ecologists to address both community composition and the interactions between species~\citep{Roslin2016}. Identifying these interactions is a crucial step towards understanding (and predicting) the effects that species have on each other and the response of the community as a whole to external perturbations~\citep{Bartomeus2016,Giron2018}. Importantly, information is conveyed both by the presence and \emph{absence} of links between species. Presences and absences are not, however, equally certain. An observed link definitely occurred, but there are multiple reasons why a given link may not be observed~\citep{Jordano2016}. 


    First and foremost, ecological communities are known to be stochastic~\citep{Gotelli2000}. Community composition and abundances vary over space~\citep{Baiser2012} and time~\citep{Olesen2011a}, leading to variation in encounter probabilites and hence in interactions~\citep{Vazquez2005,Poisot2015,Graham2018}. Species must co-occur in order to be observed interacting, so any variation in co-occurrence will create variation in the set of observed interactions~\citep{Gravel2013,Graham2018}. Even assuming constant co-occurrence within a given study system, changing abundances can affect interactions if species which would otherwise interact become too rare to detect each other~\citep{Tylianakis2010,Jordano2016}. Moreover, interactions vary over space~\citep{Kitching1987,Baiser2012,Emer2018}, time~\citep{Kitching1987,Olesen2011a,Lopez2017}, between individuals~\citep{Pires2011a,Fodrie2015,Novak2015}, throughout life cycles~\citep{Preston2014,Clegg2018}, and with environmental conditions~\citep{Poisot2015}.


    Beyond ``true" variation in network structure, several researchers have noted the effects of sampling intensity (e.g.,~\citealp{Martinez1999,Bluthgen2006,Bluthgen2007,Jordano2016}). An assessment of the accumulation of inter0actions with increasing sampling effort suggests that it is even more challenging to correctly document interactions than species~\citep{Guimera2009,Poisot2012,Bartomeus2013,Jordano2016,Giron2018,Graham2018}. As a result, it has been proposed that interactions should be described probabilistically and network metrics computed accordingly~\citep{Bartomeeus2013,Poisot2016}. Efforts in this vein usually take the form of creating model food webs that approximate observed empirical networks~\citep{Allesina2008,Guimera2009,Williams2010,Rohr2016}. These models may be based on species traits and/or abundances~\citep{Rohr2016,Weinstein2017,Weinstein2017a,Graham2018} or on simple abstract rules~\citep{Allesina2008,Guimera2009,Williams2010}. Such models provide an important ''reality check'' by allowing us to test hypotheses about the factors structuring ecological networks~\citep{Bartomeus2013,Weinstein2017.Weinstein2017a,Graham2018}. They can also allow us to estimate the numbers of interactions which have not been observed~\citep{Jordano2016,Weinstein2017a} and predict which unobserved interactions are most likely~\citep{Guimera2009,Bartomeus2013}. 


    [[These two paragraphs try to frame our study more clearly. Any help to shorten them?]]
    To date, models incorporating uncertainty have either been very general (e.g.,~\citet{Guimera2009,Gravel2013}) or very system-specific (e.g.,~\citet{Bartomeus2013,Weinstein2017,Weinstein2017a,Graham2018}). The general models may not include any system-specific information, while the system-specific models may not be applicable outside of their system of origin. Here we propose a simple Bayesian framework that can include broad general trends in network structure or highly specific expert knowledge, depending on the situation. This framework is designed to make the assumptions used when building networks more explicit and to quantify our uncertainty around each interaction in a network. Importantly, the simplicity of the framework means it should be easily applicable to any network type or study system.


    First, we provide a brief description of the nested levels of uncertainty affecting ecological network construction. In the context of this uncertainty, which can be reduced but usually not eliminated by high-quality sampling (\citealp[see Box 1]{Bartomes2013}), we present our Bayesian framework for combining observed data with a prior expectation of network structure. We conclude with a worked example applying this framework to an intensively-sampled willow-galler network. We find that even the highest-quality empirical network we could find had substantial uncertainty about whether many pairs of species interact, demonstrating the need to include information about this uncertainty in published networks. We also show that this interaction-level uncertainty leads to uncertainty about estimates of network structure, with important implications for comparisons across networks.


    % Development of molecular sampling methods such as DNA barcoding hold great potential for uncovering interactions missed by other sampling methods (such as visit surveys)~\citep{Giron2018}, but these novel methods are also somewhat error-prone at present~\citep{}. 

    % Accounting for differences in feasible (binary) interactions across life stages changes network structure~\citep{Clegg2018}.

    % Representing an ecological community as a network summarises both species composition and interactions between species. A tabulation of the nodes (species) and their relative abundances forms the basis for traditional metrics of community composition such as alpha diversity. A network framework combines this tabulation of nodes with a list of interactions (links between nodes) so that networks provide additional, higher-order information on community structure. Despite this additional information, empirical descriptions of ecological networks are still often limited by a lack of data or tools to adequately incorporate variation in interactions into networks. That is, whether the network is assembled based on aggregated data, a single intensive ``snapshot" sample, or expert knowledge, interactions are assumed to occur deterministically wherever and whenever the community is observed~\citep{Olesen2011a}. 


    % The assumption of static communities contrasts with the known stochasticity of ecological communities~\citep{Gotelli2000}. Community composition and abundances vary over space~\citep{Baiser2012} and time~\citep{Olesen2011a}. Moreover, interactions vary over space~\citep{Kitching1987,Baiser2012}, time~\citep{Kitching1987,Olesen2011a}, and between individuals~\citep{Pires2011a,Fodrie2015,Novak2015}. Worse, variability in community composition and interactions may not be closely related. The removal of a species from a site will obviously also remove its interactions but, conversely, the co-occurrence of potentially interacting species does not guarantee that the species will interact at a given place and time. Interactions can be lost if the interaction partners remain present but are separated in time or are too rare to detect each other~\citep{Tylianakis2010}. Interactions can also fail to occur because of environmental contingencies~\citep{Poisot2015} or individual preferences~\citep{Fodrie2015}. 


    % Beyond ``true" variation in network structure, several researchers have noted the effects of sampling intensity (e.g.,~\citealp{Martinez1999,Bluthgen2006,Bluthgen2007}). An assessment of the accumulation of interactions with increasing sampling effort suggests that it is even more challenging to document interactions than species~\citep{Poisot2012,Bartomeus,others}. As a result, it has been proposed that interactions should be described probabilistically and network metrics computed accordingly~\citep{Poisot2016}. Early work in this vein includes food-web models using likelihood-based approaches~\citep{Allesina2008} or Gaussian~\citep{Williams2010} or binomial~\citep{Rohr2016} probability functions for each possible interaction. These models may include information about species' traits~\citep{Rohr2016} or attempt to reproduce empirical network structures using a set of simple rules~\citep{Allesina2008,Williams2010}.


    % Despite these efforts, we currently lack the quantitative methodology to deal with uncertainty generated by spatiotemporal variation in ecological interactions and by limited sampling. [[cite previous attempts]] Even in well-sampled networks, uneven sampling across species can lead to the inference that some species do not interact because they co-occur rarely or have not yet been observed together - even if they do interact when they do co-occur (see Fig.~\ref{histograms}). Nearly all network studies will thus neglect some interactions, necessitating an approach that acknowledges this uncertainty.


    % In this study, we formalise the description of interactions between species as probabilities and develop analytical tools to capture the uncertainty in the estimation of these interactions. We focus on binary interactions as a first step, but the framework could be expanded to deal with interaction frequencies or strengths. To define the problem, we first identify different contributions to the uncertainty of an interaction and discuss the implications of each source of uncertainty for the properties of ecological networks. 


    \subsection*{Why do some interactions \emph{not} occur?}

      We start from the perspective of a community ecologist faced with the task of describing a previously unknown interaction network. This ecologist will be interested in generating a description of the species/nodes present and the links between them~\citep{Roslin2016}.  Importantly, the information sought is conveyed by both the presence and \emph{absence} of links. Presences and absences are not, however, equally certain. An observed link definitely occurred, but there are multiple reasons why a given link may not be observed \emph{whether or not the interaction truly occurred}. In terms of the ones and zeros in a binary interaction matrix, we know that the ones are true but we cannot be sure that the zeros reflect interactions which actually do not occur. The detection of any interaction is a stochastic process. As a conceptual guide to the factors affecting this process, we describe three nested levels of uncertainty that roughly address the questions: "Could species $i$ and $j$ interact?", "Do they interact?" and "Do we observe the interaction?" (Fig.~\ref{conceptual_fig}).


        \subsubsection*{Could the species interact under ideal conditions?} 

          First, and most fundamentally, some interactions are feasible if a pair of species should co-occur while others are not~\citep{Poisot2015}. Assuming that the feasibility of interactions depends on the traits of species involved, we can define the probability of an interaction $L$ given some description of trait-matching $\mathbf{T}$ as $P(L|\mathbf{T}) = \lambda$~\citep{Gravel2013,others}. 
          In some cases the traits prohibiting an interaction are known, and there will be little uncertainty about $\lambda$.
          In most cases, however, it is not known what traits might prohibit an interaction~\citep{Dormann2017}. Pollinators may visit plants outside of those predicted based on "pollination syndromes"~\citep{Weinstein2017a} and herbivores may consume animal prey when it becomes accessible~\citep{}. These examples show that we often cannot say that an interaction absolutely does not occur. Instead, an unobserved interaction may be a rare phenomenon with $\lambda>0$ and the associated zero in an interaction matrix might be false. This uncertainty is partly addressed by trait-matching models in which species with better-matched traits are more likely to interact~\citep{Bartomeus2016,Jordano2016}. As these models improve and include better information about the influence of traits on interactions, the uncertainty will be reduced~\citep{Jordano2016} and $\lambda$ should either tend to 0 or to 1. Nevertheless, every model is imperfect and lacks information that could be used to define constraints on at least some interactions~\citep{Dormann2017}. Until these gaps are filled, some uncertainty will remain about whether some species pairs could interact.


        \subsubsection*{Did the species interact during sampling?} 

          Assuming that an interaction is feasible (i.e., $L=1$;), an interaction may still not occur at a particular place or time~\citep{Poisot2015,Graham2018}. This could be because the two species do not co-occur or, if they did occur, because constraints such as inclement weather or low abundance prevented the interaction~\citep{Jordano2016,Graham2018}, a more preferred interaction partner was available~\citep{Weinstein2017a}, or substantial individual variation in traits meant that local individuals could not interact even if the species in general can~\citep{Gravel2013,Poisot2015}. For example, a rare galler may not parasitize a rare plant because they do not encounter each other. 
          We can define the local realisation of an interaction, $X$, given that the interaction is feasible, as a stochastic process with associated probability $P(X|L=1)=\chi$. It is unlikely that $\chi=1$  except in the case of obligate specialists who always interact whenever they co-occur and always co-occur. When $\chi<1$ we cannot be sure whether an unobserved interaction did not occur and when it occurred but was not observed.

          This phenomenon of interaction contingencies is usually not considered in network studies, but there is a rich literature in community ecology  on the topic. Phenological matching~\citep{MillerRushing2010,Gezon2016}, species preferences~\citep{Pires2011,Novak2015,Coux2016}, and fear effects~\citep{Luttbeg2005,Wirsing2008} are just some of the factors contributing to variation in the frequency of interactions between a given pair of species. Some of these factors can be addressed in mesocosm studies of networks (e.g., environmental conditions can be held stable) or could be included in models along the line of trait-matching models (e.g., phenological matching). Expanding sampling to cover a broader spatial or temporal range (e.g., sampling in a variety of microhabitats or during a variety of weather conditions) will also help to reduce uncertainty if resource constraints permit. As long as researchers wish to sample empirical networks subject to changing conditions, however, it is not likely that they will be able to eliminate all uncertainty about whether or not pairs of species will interact during a given sampling period.


          Some models attempt to eliminate the sources of variation above by sampling intensively over a short time, during which probabilities of interaction are assumed to be constant~\citep{Bartomeus2013,Weinstein2017,Weinstein2017a}. Any remaining variation in the interactions observed is therefore assumed to be due to detection uncertainty. This assumption may be accurate for some systems where the environment is relatively constant, there is little intraspecific variation in traits or preferences, etc. In most systems, and for any effort to compile a metaweb that will apply over broader spatial and temporal scales, probabilities of interaction are likely to vary during sampling.

         % One proposed solution to the problem of process uncertainty is to use repeated sampling of networks. For example,~\citet{Weinstein2017-wrong one?} used repeated sampling to estimate the daily probability of detecting an interaction and thereby attempted to separate detection and process uncertainty.[[This apparently confused R3]] While conceptually attractive, this approach is unsuitable for interactions occurring over longer time scales (e.g., associations between hosts and parasitoids with a single generation per year), which would require multi-year sampling efforts, or very rare interactions which would require extremely high sampling effort to detect. Moreover, the faster accumulation of species than interactions means that repeated sampling will tend to introduce more unobserved interactions than observed interactions unless the number of samples is high enough to ensure that all species present have been observed~\citep{Poisot2012,Jordano2016}.

          \textbf{Forbidden links}

            Both of the above sources of uncertainty (i.e., whether species would be able to interact given their traits and whether they are able to interact at a particular place and time) are addressed in the concept of "forbidden links"~\citep{Jordano2016}. In this framework, all links which are prevented by spatio-temporal uncoupling~\citep{Jordano1987}, physiological constraints~\citep{Jordano1987,More2012}, etc. are considered "structural zeros" that cannot be observed~\citep{Jordano2016}. We conceptually distinguish between physiological constraints and spatio-temporal uncoupling to allow for cases in which a species is introduced to a new habitat, expands its range, or shifts phenology~\citep{Gravel2013}. In such cases, links which are forbidden due to physiological constraints remain forbidden but links previously forbidden due to spatio-temporal mismatch could potentially occur. 


        \subsubsection*{Detection uncertainty} 

          Lastly, measurement errors are a pervasive source of uncertainty in the observation of ecological processes. Even if an interaction is feasible and occurs during sampling ($L = 1$ and $X = 1$), we may still fail to detect it~\citep{Jordano2016,Weinstein2017}. We may define the probability of detecting of an interaction, $D$, given the interaction is feasible and occurs, as a stochastic process with the associated probability $P(D|X=1,L=1)=\delta$. Detection failure could happen for many reasons (see~\citet{Wirta2014} for examples and partial solutions to them). Some sources of detection error can be minimised with appropriate sampling effort (e.g., combining plant-focused and animal-focused sampling when assembling pollination networks~\citet{Jordano2016}), but other sources are often difficult to reduce (e.g. the occurrence of cryptic species might require molecular analysis for appropriate taxonomic identification as in~\citealt{Wirta2014,Frost2016}). Some types of interactions will have higher detection uncertainty than others (e.g., pollination or predation often take place very quickly while parasitism can last for months or years). Even long-term interactions, however, can be missed, especially if species are difficult to identify or if not all individuals of a species share the interaction. Parasites, for example, are often concentrated in only a few individuals~\citep{Lagrue2017}. If these infected individuals do not happen to be included in a sample, their interactions will be missed.


      To sum up, the absence of an interaction on a given day could  be because it was not feasible [$P(L) = 0$], was feasible but did not occur because of local conditions during sampling ($P(X|L=1) = 0$], or was feasible and occurred during sampling but could not be observed [$P(D|X=1,L=1)=0$]. Note that some exteral factors can affect more than one level of uncertainty. For example, rare species are less likely to interact at a given site due to neutral processes~\citep{}. Interactions involving rare species are also less likely to be detected, unless more sampling effort is focused on rare than common species~\citep{Jordano2016}. Thus, abundance affects both the probability of an interaction occurring at a particular site \emph{and} our ability to observe the interaction. Similarly, bad weather can make interactions such as pollination less likely by reducing pollinator activity~\citep{}. For those pollinators which do venture forth in poor conditions, bad weather might also  increase detection uncertainty through reduced visibility or if pollinators make shorter trips. The "forbidden links" of~\citet{Jordano2016} can also occur due to process or interaction uncertainty in our framework. 


    \subsection*{Estimating detection and process uncertainty}

        An observed interaction indicates that an interaction was feasible, occurred during sampling, and was detectable ($L = 1$, $X = 1$, and $D = 1$). An unobserved interaction could be due to any of the three sources of uncertainty ($D$, $X$, and/or $L = 0$) and it is not usually possible to identify the source in empirical data. Only the case where $L = 0$ is an interaction that never occurs --the usual interpretation of a $0$ value in an interaction matrix. It is particularly important to rule out the situations where $D=0 \cup X = 1 \cup L=1$, i.e. where the interaction occurred at the location but was not observed, and $X = 0 \cup L =1$, i.e., where the interaction is feasible but did not occur at the local site (Fig.~\ref{conceptual_fig}). An empirical ecologist will instead measure the marginal probability $P(L) = k/n$, where $k$ is the number of observed interactions and $n$ the number of observed co-occurrences. Given this information, the question becomes: how can we reduce the uncertainty around our estimated interaction probability?


        The obvious rule of thumb to reduce uncertainty is ``sample more". Increasing sample sizes will reduce uncertainty about the upper bound of the interaction probability and will also increase the probability of detecting unlikely or cryptic interactions (e.g., interactions where $L$=1 but process or detection uncertainty is high). Despite these benefits, we note that there are limits to the utility of increased sampling. Since the probability of observing the co-occurrence of two species will always be higher than the probability of observing their interaction (since the probability of interaction is conditional on both interaction partners being present), we will accumulate observations of co-occurrences faster than we will accumulate observations of interactions (Fig.~\ref{histograms}C). Thus, the more we sample, the more zeros will appear in our interaction matrix.


        An added complication is that increased sampling will not reduce uncertainty evenly across interactions. To record an interaction between A and B, we need to identify both partners correctly (a non-trivial problem in many food webs; e.g.~\citealp{Kaartinen2011,Roslin2016}). For both molecular and rearing techniques, certain types of interactions may go unnoticed due to technical challenges~\citep{Wirta2014}. This can bias the set of recorded interactions towards those that are easier to observe~\citep{Carstensen2014,Jordano2016}.


        Most importantly, if two species are never observed co-occurring during several days of sampling then repeated sampling reveals nothing about their probability of interacting should they ever co-occur. Intensive sampling is, therefore, an important step towards reducing uncertainty in ecological networks but is unlikely to be enough in most cases. 
        Indeed, despite substantial improvements in recent decades, most available interaction networks are still undersampled and, moreover, are only subsets of larger communities~\citep{Jordano2016}.
        To supplement repeated sampling efforts and reduce uncertainty around unobserved interactions, we suggest researchers use prior knowledge of the same or similar systems. Here, we outline a Bayesian approach to combining repeated sampling and prior knowledge in probabilistic networks. We then demonstrate how to construct such a probabilistic network using an intensively and repeatedly sampled empirical dataset. Finally, we show how a probabilistic understanding of a network allows us to establish confidence intervals around measures of network structure.


\section*{Materials and methods}

  \subsection*{A Bayesian framework for interaction probabilities}

    % Maybe we can move much of this to the Box and satisfy reviewer 1


    % We are most interested in how to quantify an interaction probability, and its associated uncertainty, for interactions that have not been observed. For interactions which have been observed, methods already exist to quantify the probability of the interaction. 


    % Assume that a pair of species has been observed co-occurring $n$ times and interacting in $k = 0$ cases. We want to know the probability $\lambda$ that the species would be observed interacting if not constrained by local conditions, imperfect detection, etc. We can consider the occurrence of an interaction as a Bernoulli trail where the number of successes ($k$) over $n$ trials will follow a binomial distribution (see~\emph{Appendix S2} for a full description of the mathematical framework). In this framework, the maximum likelihood estimate (MLE) for $\lambda$ is:

    %     \begin{equation}
    %       \lambda_{MLE} = \frac{k}{n}  .
    %       \label{theta_MLE}
    %     \end{equation}
  

    % Note that $\lambda$ is not a point estimate but rather a random variable with an unknown distribution. This means that if $k = 0$ in a given sample, this does not necessarily imply that the two species will never interact. Rather, $k = 0$ implies that `no interaction' is the most likely outcome when the species do co-occur but there is nonetheless some chance that the two species \emph{could} interact. In the situation where $k>0$, in contrast, we are sure that the interaction is feasible ($L = 1$). If $n>k>0$, the interaction is feasible but there may be local constraints ($X = 0$) or detection errors ($D<1$) causing the interaction not to be observed in some samples. 


    % To properly interpret $\lambda$, we need to estimate the variance as well as the MLE. The variance of a Bernoulli experiment is $n\lambda$(1-$\lambda$) and, importantly, describes the variability of the number of successes $k$ for $n$ trials rather than the variance associated with the estimation of $\lambda$. It is, however, straightforward to compute the confidence interval for the MLE of $\lambda$ using any of several methods, including the \emph{Wilson score interval}, the \emph{Clopper-Pearson interval}, and the \emph{Agresti-Coull interval} (for details, see [\citealp{Brown2001}]). 


    % All of the above methods for estimating the variance of $\lambda$ include the number of samples $n$. This means that where the number of samples $n$ is very low (e.g., for rare species), there will be considerable uncertainty around our estimate of $\lambda$. In Fig.~\ref{upper_limits}, we derive the Clopper-Pearson interval to explore how the estimate of $\lambda$ (for true $\lambda = 0$) varies with sample size. At a small sample size, the 95\% confidence interval is nearly (0, 1). To establish that species are not interacting with any acceptable certainty requires tens of observations of the two species co-occurring but not interacting. Most data sets will lack such extensive sampling across all species pairs, but we can still use a Bayesian approach to supplement the available data with other sources of information.


    We can create a simple Bayesian approach to modelling the probability of observing an interaction ($\lambda$) by combining a maximum likelihood estimate (MLE) of $\lambda$ with a prior distribution (described in the next section) and a normalising function. The MLE of $\lambda$ can be modelled as a Bernoulli trial based on the number of observed interactions $k$ and observed co-occurrances $n$: $\lambda = \frac{k}{n}$. The most appropriate prior distribution for a probability such as $\lambda$ is the beta distribution:

    \begin{equation}
          \lambda \sim Beta(\alpha,\beta) , \label{prior}
        \end{equation}

        \noindent which has two shape parameters, $\alpha$ and $\beta$. 

    Combining this prior distribution with the MLE of $\lambda$ derived from observed data, we obtain a posterior distribution that also follows a beta distribution with new parameters $\alpha'= \alpha+k$ and $\beta'=\beta+n-k$ (see~\emph{Appendix S2} for details). The weight of the prior on the posterior distribution can be understood from these parameter definitions: the difference between the posterior and the prior will increase with $k$ and $n-k$. In other words, the distribution of $\lambda$ for better-sampled pairs of species will rely less on the information used to build the prior distribution and depend more on the observed data and uncertainty about the probability of the pair interacting will decrease (Fig.~\ref{Salix_pdfs_cdfs}). 

    % R1 suggests removing this - could be in the SI I guess?
    % \subsubsection*{Moments and other properties}

    %   It is common to perform analyses that require calculating higher-order network properties in interaction networks. The fact that the posterior distribution of $\lambda$ follows a beta distribution makes it straightforward to compute moments and other properties needed for these analyses. 


    %   The \textbf{average} of $\lambda$ is: 
    %       \begin{equation}
    %         \bar{\lambda} = \frac{\alpha+k}{\alpha+\beta+n} ,
    %         \label{mean}
    %       \end{equation}

    %     and its \textbf{variance} is:  
    %       \begin{equation}
    %         Var(\lambda|k) = \frac{(\alpha + k)(\beta + n - k)}{(\alpha + \beta + n)^{2}(\alpha + \beta + n +1)}
    %         \label{variance}
    %       \end{equation}

    %     The \textbf{mode} of the distribution is:
    %       \begin{equation}
    %         \hat{\lambda} = \frac{\alpha + k - 1}{\alpha + \beta + n - 2} .
    %         \label{mode}
    %       \end{equation}


    \subsubsection*{Choosing a prior distribution}  [[expand]]
      [[expand]]

      Parameters $\alpha$ and $\beta$ determine the shape of the prior distribution of $\lambda$, which follows a beta distribution. These are called hyper parameters. The prior may be chosen to be uninformative, if no external information is available, or may draw on information such as distributions of connectance across webs, degree distributions, or trait-matching functions (see \emph{Appendix S3} for details and a simple quantitative example). 


      It is important to ensure that an informative prior accurately reflects the focal  system. For example, if the system is a large network containing many plant species and insects from different families, it is inappropriate to use a prior distribution drawn from a small network describing interactions between different genotypes of a single plant species and a set of closely-related insects. The similarity of traits across genotypes within a species means that an insect which interacts with one genotype is likely to interact with many others. Trait differences between species mean that a species-species network will likely contain fewer interactions than a genotype-species network. Similarly, highly-aggregated networks containing nodes which represent whole classes or families are likely to have very different structures than networks where most nodes represent single species as aggregated nodes are likely to have more interaction partners than single species.


      The choice of prior will depend on the information available and the question of interest. In general, it is best to use a prior taken from a similar system as emphasised above. If such data are not available, a prior based on many networks with a similar level of resolution could be useful, as could a prior based on a preliminary round of sampling (analogous to ``training data"). If there is no suitable informative prior, it is best to use an explicitly uninformative prior. While an uninformative prior will not greatly reduce variance about $\lambda$, it does clearly state our lack of prior knowledge and will not bias our conclusions. Similarly, if detailed information is available but refers to the question at hand, it is best to use a less informative prior so as not to force a particular result. For example, if researchers are interested in testing whether body-size ratios affect probabilities of interaction they should not use a prior which predicts interactions based on body sizes.

      \textbf{Uninformative priors}

        An uninformative prior is chosen so as not to bias the posterior distribution in any particular direction. That is, all of the information will come from the observed data. For pairs of species with little or no observed data, an uninformative prior will leave large variance about $\lambda$ and reflect our lack of knowledge about the pair. Since we model $\lambda$ as a Bernoulli trial, the appropriate uninformative prior is Jeffreys prios: a beta distribution with $\alpha$=$\beta$=0.5. The sum of $\alpha$ and $\beta$ is roughly equivalent to the weight of the prior, so Jeffreys prior has equal weight to a single observation. An uninformative prior is most appropriate when considering a novel system or a system where the only prior knowledge available directly relates to the question we wish to analyse.


      \textbf{Informed priors}

        In most cases, researchers will be able to draw upon prior knowledge for at least some interactions. One very simple example is the assumption that, in a food web, plants will not consume any prey. There is very little uncertainty around the lack of a observed predation of any plant upon any animal in the system, regardless of how often the plant and animal are observed together. 

        [[addressing different types of uncertainty]]
        [[drawing upon system-specific or broader knowledge]]

        If we are interested only in interactions which occur in the focal system under current environmental conditions, we might use a prior which includes probabilities of co-occurrence. That is, we could limit the set of potential interactions to pairs of species which currently co-occur, condition probabilities of interaction on probabilities of co-occurrence based on abundance or habitat use, etc.~\citep{Gravel2013,Weinstein2017} Note that such models lump together interaction uncertainty and process uncertainty and are only concerned with measuring detection uncertainty. Whether or not this is appropriate will depend upon the question of interest.


        If traits governing interactions are known (e.g., body size in food webs~\citep{Gravel2013} or corolla depth in pollination networks~\citep{Weinstein2017a,Graham}), then trait-based models may be used as a prior. Such models address interaction uncertainty directly (i.e., they estimate whether a pair of species \emph{can} interact, regardless of whether they are likely to encounter each other). To address both interaction and process uncertainty, a researcher might combine trait-based and co-occurrence models into a single prior. Such a model would reduce the uncertainty about unobserved interactions, but at the cost of adding more assumptions about the study system. This type of very detailed prior is therefore most appropriate in very well-understood systems where a great deal of expert knowledge can be applied, or in systems which are believed to follow general rules of, for example, allometric scaling~\citep{Gravel2013}. Where there is less system-specific prior knowledge, we can use prior models based on general trends in allometric scaling~\citep{Riede2011,Gravel2013}.


        An informed prior could be based on one or a few traits, as those described above, or it could incorporate a great deal of information about co-occurrence and encounter probabilities, forbidden links, etc.~\citep{Jordano2016}. Importantly, the complexity of a prior model depends both on the information available and on the question of interest. If researchers wish to test for an effect of body mass ratios on interaction probabilities, for example, they should not include body mass in their prior. Including a trait in a prior means assuming that trait has an effect on which interactions we detect and it is therefore inappropriate to test for such an effect in the same study.


        The relevant traits and degree of trait-matching between interaction partners is likely to vary between systems~\citep{Gravel2013,Weinstein2017a}. We therefore urge caution when using a model developed for one system as the prior for another. 


      \textbf{Empirical priors} [[using data from the data in question]][[contentious]][[should these be drawn from hyperpriors or fixed for each interaction]]


  \subsection*{An empirical example}
    [[Make the point of this clearer]]
      To illustrate the process of constructing a Bayesian network, we use the comprehensively-sampled system of willows (\emph{Salix}), herbivorous gallers, and their natural enemies described by~\citet{Kopelke2017}. This dataset consists of a single community type sampled across Europe over 29 years and at 374 unique locations. The meta-network consists of 1,173 different interactions between 52 \emph{Salix} nodes, 92 herbivore nodes, and 126 natural enemy nodes (see \emph{Appendix S4} for details). 


      The high spatiotemporal resolution and unusually high sampling effort of this dataset make it well suited for illustrating the difficulties in completely sampling a network. To show the gaps in sampling, we compared the frequencies of observed co-occurrences and interactions. We analysed both the \emph{Salix}-galler and galler-natural enemy components of the network but, for brevity, present only the latter here (see~\emph{Appendix S5} for \emph{Salix}-galler results). 


      Unlike~\citet{Bartomeus2013,Weinstein2017,Weinstein2017a}, our model can include interactions for species which were observed infrequently (or not at all) during sampling. 

    \subsubsection*{Computing the prior and posterior distributions}

        To treat each interaction as a Bayesian probability, we will combine the observed data with a distribution based on prior information. A strict Bayesian framework requires a prior distribution that does not rely on any information from the study at hand. If such data are not available, one might instead use the first sub-network collected as ``training data" to guide future sampling. To simulate this situation, we created priors using a single sub-network from the middle of the geographical distribution of the~\citet{Kopelke2017} dataset. To demonstrate how the use of data from a different system can affect the prior distribution and conclusions based on it, we repeated our analyses using priors derived from a much smaller \emph{Salix}-galler-natural enemy system based around genotypes of a single \emph{Salix} species~\citep[Data available from the Dryad Digital Repository: https://doi.org/10.5061/dryad.g7805]{Barbour2016}\nocite{Barbour2016Dryad}. This smaller system was much more densely-connected than that described in~\citet{Kopelke2017} and provided unreasonable distributions for interaction probabilities (\emph{Appendix S6}). 


        To obtain priors based on a single sub-network --Zillis in Graub\"{u}nchen, Switzerland-- we estimated frequencies of interactions based on the normalised degree of each species in each network component (see \emph{Appendix S7} for details and code). Using these prior parameters, we then estimated the posterior distributions of interaction probabilities $\lambda$. For species without observed interactions ($n = 0$), the posterior distribution is identical to the prior distribution. For species where $n>0$, we can update the prior distribution with data. If we consider only pairs of species which were observed co-occurring but not interacting, $k_{ij}$ is always 0 and only $n_{ij}$ will vary between species pairs. This gives $\alpha'$=$\alpha$ and $\beta'$=$\beta + n_{ij}$. We calculated posterior distributions and 95\% credible intervals (see function ``credible\_interval";~\emph{Appendix S8}) for species with $n$ ranging between 0 and 374, the total number of sites in our dataset. 


        Rather than calculating credible intervals for a posterior distribution after collecting data, we may wish to know how many data points are necessary to obtain a given level of confidence that two co-occurring species do not interact. The number of samples needed will depend on both our desired level of confidence and the threshold below which we assume that two species are unlikely to ever interact. We calculated the number of samples required to reach 95\% confidence that $\lambda$ was below thresholds of 0.1, 0.05, and 0.01 as examples (see function ``samples\_for\_threshold";~\emph{Appendix S8}).


    \subsubsection*{Scaling up to network metrics}

      Researchers usually use interaction matrices as a tool to calculate measures of network structure. Computing most network metrics is straightforward when the different $\lambda$ of the adjacency matrix are known and assumed not to vary~\citep{Poisot2016}. Incorporating variance in $\lambda$ into these calculations, however, is not so easy. Computation of these metrics involves non-linear functions. By Jensen's inequality~\citep{Jensen1906}, this means that the average of a network metric (a function of stochastic interactions) is not the same as the network metric calculated based on the average interaction probability. Any uncertainty in the values of $\lambda$ could bias both the mean and variance of network metrics, giving misleading results (simulated in \emph{Appendix S1}). One way to avoid this situation is to calculate the properties of a suite of simulated networks.


      Using the prior distributions and procedures described above, we calculated posterior probability distributions for species pairs that were not observed interacting. Using these posterior distributions and assuming probabilities of 1 for pairs of species that were observed interacting, we created a suite of 100 webs by randomly sampling from each posterior distribution. After obtaining these posterior networks, we calculated the connectance of each web, as well as the mean number of links per galler and per natural enemy, and the nestedness (NODF) of the network. To demonstrate how these network metrics will be affected by detection uncertainty, we then created a suite of filtered networks for each posterior network. Networks were filtered by randomly sampling 99\%, 95\%, 90\%, 80\%, 70\%, 60\%, and 50\% of the interactions included in each posterior network. This gradient is akin to a gradient of sampling effort. For each level of detection accuracy, we created 100 randomly-sampled networks per posterior-probability network (giving 100 posterior networks and 1000 detection-filtered networks). We calculated the same network properties as described above for all posterior and detection-filtered networks.


\section*{Results}

  In the \emph{Salix}-based food webs sampled by~\citet{Kopelke2017}, most pairs of gallers and natural enemies (9,794/12,096) never co-occurred and, for species that did occur together, the total number of co-occurrences was generally low (mean=3.87; Fig.~\ref{histograms}A). The bulk (92.24\%) of these co-occurring species pairs were never observed interacting. Of those pairs that did interact, the incidence of interaction was also low (mean=4.04; Fig.~\ref{histograms}B) and was lower than the number of observed co-occurrences (Fig.~\ref{histograms}C).


  We obtained prior parameters of $\alpha$=0.700, $\beta$=8.49. Where $n = 0$, these parameters gave a posterior distribution with $\bar\lambda$=0.076, var($\lambda$)=0.008. When $n = 374$, we obtained a posterior distribution with $\bar\lambda_{ij}$=1.83 $\times$ 10$^{-3}$, var($\lambda_{ij}$)=4.76. This distributions is very close to 0 with small variance about $\lambda$; if species $i$ and $j$ co-occurred 374 times without interacting, they   are extremely unlikely to do so at other sites or times. 


  For most pairs of species $i$ and $j$, $n_{ij}$ was much less than 374 and our posterior mean and variance therefore retain more of the influence of the prior. We can see this in the increasing means and variances as we decrease $n_{ij}$ (Fig.~\ref{Salix_pdfs_cdfs};~\emph{Appendix S9}). To be 95\% confident that the probability of interaction is below 0.1, 0.05, or 0.01 would require 15, 39, and 229 observed co-occurrences, respectively. Note that these are relatively large sample sizes compared to currently-available empirical networks (e.g.,~\citealp{Morris2014}).


  Scaling up to network structure, we found that the connectance and mean links per galler and natural enemy were much lower in the observed web (C=0.078, $L_{galler}$=9.99, and $L_{natural enemy}$=7.45, respectively) than in the posterior webs (0.186 $\leq$ C $\leq$ 0.198, 13.4 $\leq L_{galler} \leq$ 14.6, and 23.4 $\leq L_{natural enemy} \leq$ 25.0). When the detection probability was relatively low (i.e., 50\%), however, the properties of randomised networks became similar to those in the observed webs (Fig.~\ref{posterior_webs}A,B,D). Nestedness was higher in the observed network (NODF=6.85) than in the posterior webs (6.31 $\leq NODF \leq$ 6.82; Fig.~\ref{posterior_webs}C); in this case, the stronger the detection filter, the farther apart were the observed and posterior webs. In general, the observed network is most similar to simulated networks where only half of the plausible links are detected. 

  % Poisot2015 - also found that NODF was higher in the empirical data than in sims. 

\section*{Discussion}

  Real interaction networks vary over several dimensions~\citep{Kitching1987,Olesen2011a,Pires2011a,Baiser2012,Fodrie2015,Novak2015}, leading to pervasive under-estimation of interactions in published networks~\citep{Jordano2016}. Even in the most extensive data set that we could find, there was very little empirical data for each species pair. Most pairs of species were not observed co-occurring even once, less than 10\% of species pairs were observed interacting, and no pairs were observed to co-occur frequently enough to conclude that their probability of interacting was below 0.01. This suggests that limited sampling is a major source of uncertainty in empirical networks, in agreement with~\citet{Jordano2016,Weinstein2017a,Weinstein2017}. This dataset also illustrates the problem of process uncertainty - pairs of species which were observed interacting did not interact in all of the samples in which they co-occurred. Increasing sampling is therefore likely to add more zeros than ones to an interaction network.


  Detectability estimated at 23.3\% in~\citet{Weinstein2017a}


  Using prior knowledge to build a Bayesian distribution for each interaction probability allows us to set appropriate bounds on interaction probabilities for species which were rarely or never observed co-occurring. In such cases, an informative prior distribution gives us more reasonable estimates of interaction probabilities than assuming that $p$=0 for unobserved interactions. This approach is particularly useful when considering interactions involving species entering new ranges due to climate change or introductions. Understanding these species' interactions is important for a variety of conservation questions~\citep{Bartomeus2013,Gravel2013}, and a Bayesian approach using a trait-matching model or data from species' current ranges could help us to anticipate how species will integrate into new communities. 


  Our results also demonstrate how incomplete detection of interactions scales up to affect network structure. The structure of the well-sampled network used in our case study is most similar to the structure of simulated networks including only 50\% of the interactions suggested by the posterior distribution of interaction probabilities. If our descriptions of empirical systems are missing up to half of the feasible interactions, then it is vital to acknowledge this inherent uncertainty before comparing structures across systems or relating structure to stability.


  Based on the framework described above, we can now offer some recommendations for improved descriptions of ecological interactions. Observing all interactions in a network is likely impossible~\citep{Jordano2016}, but researchers can take steps to address the uncertainty this implies. First, sample sizes should be as large as possible in order to reduce detection uncertainty. Second, researchers should acknowledge the varying levels of confidence surrounding interactions between species pairs. Including the $n$ and $k$ values for each interaction will clearly indicate which unobserved interactions are most likely to be observed with further sampling and which estimates are more reliable. Where there are strong prior expectations about pairs of species that will not interact, these should be explicitly stated so that readers know which zeros in an interaction matrix are based on observed data and which are based primarily upon expert knowledge. Third, the uncertainty around interactions should be incorporated in calculations of network properties. Re-sampling networks based on a probabilistic understanding of networks is straightforward and gives distributions for network properties rather than point estimates. This not only acknowledges the fact that interactions vary over time and space but will also facilitate comparisons between networks by adding confidence intervals to estimates of network properties. This will allow us to say whether networks have different structures \emph{and} whether those differences are greater than we would expect given the inherent variability of interactions. To facilitate the practical application of these recommendations, we provide all code used in this paper in the supplementary material. 


\section*{Acknowledgements}

  The authors thank Daniel Cartensen for fruitful discussion of the ideas in this manuscript and K\'{e}vin Cazalles for providing feedback. The authors appreciate support from the Swedish Research Council (VR) for grant \#2016-06872 (to TR) and from Formas for grant \#942-2015-1262 (to AE).


\section*{Authors' contributions}

DG designed the analytical approach. AC and DG wrote the code. AC performed statistical analyses. All authors contributed to writing and revising the manuscript.


\section*{Data accessibility}

All data used in this study have been independently published and are accessible following the references provided in text.



\end{spacing}
\clearpage

\captionsetup{singlelinecheck=false, font={stretch=2}}

\section*{Boxes}

\begin{figure}[h!]
\caption{\textbf{Why isn't sampling the solution?}
While sufficient sampling is a vital part of any scientific study, we do not think that an exhortation to "sample more" is the appropriate solution to analysing uncertain interactions. There are four main reasons for this: \\
\textbf{1)} First, and most importantly, we assume that the majority of researchers are aware that insufficient sampling leads to shoddy data and are sampling as intensively and extensively as their time and budget allow. The development and decreasing price of novel methods such as DNA barcoding and remote sensing can help field researchers to do more with less, but it remains to be seen whether these improvements will be enough to fully-describe large study systems. In the absence of new break-through sampling techniques, logistical limitations mean that allocating sampling effort is a zero-sum game. That is, allocating more sampling to reduce one source of uncertainty will remove sampling effort that could reduce another source of uncertainty.\\
\textbf{2)} High sampling effort with a constant methodology will reliably describe some interactions but not others. For example, many insect species are difficult to distinguish except by molecular methods. Non-descructive sampling (e.g., by observing insect flower-visitors) will not allow researchers to separate these cryptic species~\citep{}. Increasing the number of person-hours spent in the field will not change this situation, although of course it will allow more accurate estimations of interaction probabilities for species which \emph{can} be distinguished. Adding another type of sampling (e.g., molecular methods) would reduce the set of interactions which cannot be detected/distinguished, but it will also increase the time, resources, and expertise required to complete the study. \\
\textbf{3)} High sampling effort in a narrow spatiotemporal window will reliably describe some interactions but not others. Species abundances vary between sites and over time, and some interactions may not occur if one or both partners is rare. In addition, some interactions only occur in brief temporal windows (e.g., flowers of Arctic plants may only open for a few days~\citep{}) or when mediated by other species (e.g., consumption of some shellfish by fish only occurs when the shellfish are heavily parasitised and cannot burrow~\citep{}). Expanding sampling to cover broader ranges of space and time will expand the set of interactions that are observed, but again will increase the time and resources needed. \\
\textbf{4)}{} Observations of rare species add disproportionately more "zeros" than ones to an interaction matrix. Rare species may appear to be specialists purely because of their rarity~\citep{}. If increasing sampling effort adds many rare species to a network, but each species is observed only one or a few times, it is likely that only a fraction of the rare species' interactions will be included. Note that we should not avoid sampling rare species! It is better to identify species about which we know little as targets for future study than to ignore these species. Nevertheless, sampling which is sufficiently high to identify most species at a site but not most interactions for rare species will underestimate the number of interactions and skew the distribution of interactions to rare species. Targeted sampling of rare species would reduce this problem, but again either increase the cost of a study or take away resources from sampling more abundant species. \\
Increased or targeted sampling could improve (and, given enough resources, perhaps solve) any one of the above problems. As each places demands on the same limited resource pool, however, it is extremely unlikely that researchers will be able to sample their way out of uncertainty about interactions. To illustrate the scale of sampling needed, we can consider the observation of an interaction as a Bernoulli trial. The number of successes ($k$ observed interaction) over ($n$ observed co-occurrances) trials will follow a binomial distribution (see~\emph{Appendix S2} for a full description of the mathematical framework). In this framework, the maximum likelihood estimate (MLE) for $\lambda$ is:

        \begin{equation}
          \lambda_{MLE} = \frac{k}{n}  .
          \label{theta_MLE}
        \end{equation}
  

    Note that $\lambda$ is not a point estimate but rather a random variable with an unknown distribution. This means that if $k = 0$ in a given sample, this does not necessarily imply that the two species will never interact. Rather, $k = 0$ implies that `no interaction' is the most likely outcome when the species do co-occur but there is nonetheless some chance that the two species \emph{could} interact. In the situation where $k>0$, in contrast, we are sure that the interaction is feasible. $k$ may not equal $n$ if local constraints or detection errors prevent the interaction from being observed in each sample, but we nevertheless know that the interaction does occur.

    Importantly, we can estimate the variance of $\lambda$ as well as the MLE; available methods include the \emph{Wilson score interval}, the \emph{Clopper-Pearson interval}, and the \emph{Agresti-Coull interval} (for details, see [\citealp{Brown2001}]). Examining the changing variance about $\lambda$ with increasing sample size (Fig.~\ref{upper_limits}), we see that tens of observations are required to establish that two co-occurring species do not interact. Most datasets lack such extensive sampling for some species pairs, especially rare species which may be of particular conservation interest~\citep{Bartomeus2013}. The main advantage of the Bayesian approach we suggest here is that it can reduce uncertainty about interactions involving species with limited numbers of observations.}
\label{Box1}
\end{figure}



\section*{Figures}

  \begin{figure}[h!]
    \caption{Three nested levels of uncertainty mean that an observed interaction matrix is unlikely to capture all of the interactions that can really occur at a site. Interaction uncertainty means that some interactions that truly occur (black squares in True matrix) are less likely than others (grey squares; lighter grey is less likely) based on species traits. For example, lions are less likely to predate upon elephants than zebras. If our trait model is incomplete (e.g., if we neglect group hunting in lions), we might assume that elephants are too large to ever be prey for lions and assign this interaction a probability of 0.\\
    \indent Process uncertainty describes the fact that interactions occur with different probabilities in any given sample. We show six example networks representing interactions observed over two days at three sites. Interactions which occur during fewer sampling days are less likely to be observed.\\
    \indent Detection uncertainty reflects the fact that not all interactions are equally detectable. For example, interactions involving cryptic species are less likely to be seen.\\
    \indent After applying these three layers of uncertainty, some interactions which truly occur are very unlikely to be observed. The observed matrix (lower right) is a subsample of the true matrix, with the probability of observing each interaction depending on the interaction, process, and detection uncertainty associated with it. Note that some low-probability interactions (e.g., lion predation on zebra) are included while others (e.g., lion predation on elephants) are not. For an illustration of the way in which these levels of uncertainty combine in simulated data, and the effects of uncertainty on network structure, see \emph{Appendix S1}. Attributions for PhyloPic silhouette images are given in \emph{Appendix S1-A}.}
    \label{conceptual_fig}
    \begin{center}
    % \includegraphics*[height=.5\textheight]{figures/conceptual_fig.eps}
    % \includegraphics*[height=.5\textheight]{Figure_1.eps}
    \end{center}
    \end{figure}


  \begin{figure}[h!]
      \caption{Despite the replication in our empirical dataset, most galler-natural enemy pairs were never observed co-occurring and those that did co-occur rarely interacted. \textbf{A)} Here we show a histogram of the number of pairs of species observed co-occurring at least once. 9794 galler-enemy pairs were never observed co-occurring (not shown).
      \textbf{B)} Here we show a histogram of the number of observed interactions within pairs of co-occurring species. Species which co-occurred but never interacted are included. 
      \textbf{C)} Here we show, for each species pair, the number of observed interactions plotted against the number of observed co-occurrences. The red, dashed line indicates a 1:1 relationship between interactions and co-occurrences.}
      \label{histograms}
      \begin{center}
      % \includegraphics*[height=.6\textheight]{figures/GP_histogram.eps}
      % \includegraphics*[height=.6\textheight]{Figure_2.eps}
      \end{center}
      \end{figure}


  \begin{figure}[h!]
    \caption{Imperfect detection of interactions increases the number of samples required to be confident that an interaction never occurs. Assume that two species cannot interact (i.e., $\lambda=0$) and that the number of observed interactions follows a binomial distribution. Here we show the upper bound (solid black line) of a 95\% Clopper-Pearson true credible interval for the interaction probability $\lambda$ for a pair of species that has been observed co-occurring $n$ times but never interacting ($k = 0$). The upper limit of the credible interval only reaches 0.1 (dashed, red line) with 35 observations. Thus, adding more observations is useful in controlling uncertainty, but the number of observations added must be very high. }
    \label{upper_limits}
    \begin{center}
    % \includegraphics*[width=.8\textwidth]{figures/upper_limit_DG.eps}
    % \includegraphics*[width=.8\textwidth]{Figure_3.eps}
    \end{center}
    \end{figure}


  \begin{figure}[h!]
    \caption{Using prior distributions based on a single site (Zillis in Graub\"{u}nden, Switzerland) from~\citet{Kopelke2017}, we calculate posterior distributions for the probability of interaction ($\lambda$) between two species that have not yet been observed interacting ($k = 0$). 
    Posterior distributions (curves) and 95\% credible intervals (lines at top of panel) for $\lambda$ narrow and approach zero as the number of observed co-occurrences ($n$) increases. Diamonds indicate the maximum likelihood estimator for the mean probability of interaction.
    Dashed lines indicate threshold probabilities of 0.01, 0.05, and 0.1. The number of samples required to obtain a 95\% credible interval below each threshold increases rapidly. It takes just over 20 observed co-occurrences to be 95\% confident that $\lambda<0.10$, approximately 50 co-occurrences to be 95\% confident that $\lambda<0.05$, and over 100 co-occurrences to be 95\% confident that $\lambda<0.01$.}
    \label{Salix_pdfs_cdfs}
    \begin{center}
    % \includegraphics*[width=.8\textwidth]{figures/GP_pdfs_increasing_N_Zillis.eps}
    % \includegraphics*[width=.8\textwidth]{Figure_4.eps}
    \end{center}
    \end{figure}


  \begin{figure}[h!]
    \caption{Mean connectance, links per galler, nestedness (NODF), and links per natural enemy for networks assembled using posterior distributions based on a single site (Zillis in Graub\"{u}nden, Switzerland) from~\citet{Kopelke2017} occupied a narrow range. To obtain distributions of network properties, we created 100 ``posterior-sampling" networks and then, for each of these, created 100 ``detection-filter" networks by randomly sampling 50\%-99\% of the interactions included in the posterior-sampling network. This simulates imperfect detection of interactions in the field. Each point represents the mean (+/- SD) network property (e.g., connectance) obtained from a set of 100 detection-filter networks. The detection-filter networks cover a broader range of network properties than the posterior-sampling networks but the value of each property decreases as the strength of the detection filter increases. Values in the original web are indicated by dashed lines.}
    \label{posterior_webs}    
    \begin{center}
    % \includegraphics[width=.8\textwidth]{figures/GP_posterior_properties_Zillis.eps}
    % \includegraphics[width=.8\textwidth]{Figure_5.eps}
    \end{center}
    \end{figure}


\clearpage

    \bibliographystyle{ecol_let} 
    \bibliography{manual_abbrev} % Abbreviate journal titles.


\end{document}


